#scr/scr_trainer.py
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset, Subset, ConcatDataset
from torch.optim import lr_scheduler
import numpy as np
from typing import Dict, List, Tuple, Optional, Set
from tqdm import tqdm
from PIL import Image
import torch.nn.functional as F  # ğŸ’ ì¶”ê°€

from loss import SupConLoss, ProxyContrastLoss  # ğŸ’ ProxyContrastLoss ì¶”ê°€
from models import get_scr_transforms
from utils.util import AverageMeter
from utils.pretrained_loader import PretrainedLoader

# ğŸ‹ ì˜¤í”ˆì…‹ ê´€ë ¨ import ì¶”ê°€
from scr.threshold_calculator import ThresholdCalibrator
from utils.utils_openset import (
    split_user_data,
    extract_scores_genuine,
    extract_scores_impostor_between,
    extract_scores_impostor_unknown,
    extract_scores_impostor_negref,
    balance_impostor_scores,
    predict_batch,
    load_paths_labels_from_txt
)


# ğŸŒˆ í—¬í¼ í•¨ìˆ˜ ì¶”ê°€
def _ensure_single_view(data):
    """ë°ì´í„°ê°€ 2ë·° í˜•ì‹ì´ë©´ ì²« ë²ˆì§¸ ë·°ë§Œ ë°˜í™˜"""
    if isinstance(data, (list, tuple)) and len(data) == 2:
        return data[0]
    return data


class MemoryDataset(Dataset):
    # ğŸŒˆ dual_views íŒŒë¼ë¯¸í„° ì¶”ê°€
    def __init__(self, paths: List[str], labels: List[int], transform, train=True, dual_views=None):
        self.paths = paths
        
        # í…ì„œì¸ ê²½ìš° CPUë¡œ ì´ë™ í›„ numpy ë³€í™˜
        if torch.is_tensor(labels):
            self.labels = labels.cpu().numpy()
        else:
            self.labels = np.array(labels)
            
        self.transform = transform
        self.train = train
        # ğŸŒˆ dual_views ì„¤ì •: ëª…ì‹œì  ì§€ì • ì—†ìœ¼ë©´ train ê°’ ë”°ë¼ê°
        self.dual_views = dual_views if dual_views is not None else train
        
    def __len__(self):
        return len(self.paths)
    
    def __getitem__(self, index):
        path = self.paths[index]
        label = self.labels[index]
        
        # ì´ë¯¸ì§€ ë¡œë“œ
        img = Image.open(path).convert('L')
        
        # ğŸŒˆ ì¡°ê±´ë¶€ ë·° ìƒì„±
        if self.dual_views:
            # í•™ìŠµìš©: 2ë·° ìƒì„±
            data1 = self.transform(img)
            data2 = self.transform(img)
            return [data1, data2], label
        else:
            # í‰ê°€ìš©: 1ë·°ë§Œ ìƒì„±
            data = self.transform(img)
            return data, label  # ğŸŒˆ ë¦¬ìŠ¤íŠ¸ê°€ ì•„ë‹Œ ë‹¨ì¼ í…ì„œ ë°˜í™˜


class SCRTrainer:
    """
    Supervised Contrastive Replay Trainer with Open-set Support.
    ğŸ’ Enhanced with ProxyContrastLoss and Coverage Sampling
    """
    
    def __init__(self,
                 model: nn.Module,
                 ncm_classifier,
                 memory_buffer,
                 config,
                 device='cuda'):
        
        # ì‚¬ì „í›ˆë ¨ ë¡œë”© (ëª¨ë¸ì„ deviceë¡œ ì˜®ê¸°ê¸° ì „ì—)
        if hasattr(config.model, 'use_pretrained') and config.model.use_pretrained:
            if config.model.pretrained_path and config.model.pretrained_path.exists():
                print(f"\nğŸ“¦ Loading pretrained weights in SCRTrainer...")
                loader = PretrainedLoader()
                try:
                    model = loader.load_ccnet_pretrained(
                        model=model,
                        checkpoint_path=config.model.pretrained_path,
                        device=device,
                        verbose=True
                    )
                    print("âœ… Pretrained weights loaded successfully in trainer!")
                except Exception as e:
                    print(f"âš ï¸  Failed to load pretrained: {e}")
                    print("Continuing with current weights...")
            else:
                print(f"âš ï¸  Pretrained path not found: {config.model.pretrained_path}")
        
        self.model = model
        self.ncm = ncm_classifier
        self.memory_buffer = memory_buffer
        self.config = config
        self.device = device
        
        # Loss function
        self.criterion = SupConLoss(
            temperature=config.training.temperature,
            base_temperature=config.training.temperature
        )
        
        # ğŸ’ ProxyContrastLoss ì´ˆê¸°í™”
        self.use_proxy = getattr(config.training, 'use_proxy_loss', True)
        self.proxy_lambda = getattr(config.training, 'proxy_lambda', 0.3)
        self.proxy_temp = getattr(config.training, 'proxy_temperature', 0.15)
        self.proxy_topk = getattr(config.training, 'proxy_topk', 30)
        self.proxy_full_until = getattr(config.training, 'proxy_full_until', 100)
        self.proxy_min_classes = getattr(config.training, 'proxy_warmup_classes', 5)
        
        # ğŸ’ ProxyLoss ê°ì²´ ìƒì„±
        if self.use_proxy:
            self.proxy_loss = ProxyContrastLoss(
                temperature=self.proxy_temp,
                lambda_proxy=self.proxy_lambda,  # ì´ˆê¸°ê°’, ë‚˜ì¤‘ì— ìŠ¤ì¼€ì¤„ë§
                topk=self.proxy_topk,
                full_until=self.proxy_full_until
            )
            print(f"ğŸ’ ProxyContrastLoss enabled:")
            print(f"   Temperature: {self.proxy_temp} (vs SupCon: {config.training.temperature})")
            print(f"   Lambda: {self.proxy_lambda}")
            print(f"   Top-K: {self.proxy_topk}")
        else:
            self.proxy_loss = None
        
        # ğŸ’ í”„ë¡œí† íƒ€ì… ìºì‹œ
        self.proto_cache_P = None      # [C, 128] í…ì„œ
        self.proto_cache_ids = []      # ì •ë ¬ëœ í´ë˜ìŠ¤ ID
        self.proxy_active = False       # warmup í›„ í™œì„±í™”
        
        # ğŸ’ ì»¤ë²„ë¦¬ì§€ ìƒ˜í”Œë§ ì„¤ì •
        self.use_coverage = getattr(config.training, 'use_coverage_sampling', True)
        self.coverage_k = getattr(config.training, 'coverage_k_per_class', 2)
        
        if self.use_coverage:
            print(f"ğŸ’ Coverage sampling enabled: k={self.coverage_k} per class")
        
        # ğŸ’ Lambda ìŠ¤ì¼€ì¤„ ì„¤ì •
        self.proxy_lambda_schedule = getattr(config.training, 'proxy_lambda_schedule', None)
        if self.proxy_lambda_schedule is None and self.use_proxy:
            # ê¸°ë³¸ ìŠ¤ì¼€ì¤„
            self.proxy_lambda_schedule = {
                'warmup': 0.1,
                'warmup_10': 0.2,
                'warmup_20': 0.3
            }
            print(f"ğŸ’ Lambda schedule: {self.proxy_lambda_schedule}")
        
        # Optimizer
        self.optimizer = optim.Adam(
            model.parameters(),
            lr=config.training.learning_rate
        )
        
        # Scheduler
        self.scheduler = lr_scheduler.StepLR(
            self.optimizer,
            step_size=config.training.scheduler_step_size,
            gamma=config.training.scheduler_gamma
        )
        
        # Transform
        self.train_transform = get_scr_transforms(
            train=True,
            imside=config.dataset.height,
            channels=config.dataset.channels
        )
        
        self.test_transform = get_scr_transforms(
            train=False,
            imside=config.dataset.height,
            channels=config.dataset.channels
        )
        
        # ì „ì²´ ë©”ëª¨ë¦¬ ë°ì´í„°ì…‹ ìºì‹œ (íš¨ìœ¨ì„± ê°œì„ )
        self._full_memory_dataset = None
        self._memory_size_at_last_update = 0
        
        # ğŸ‹ === ì˜¤í”ˆì…‹ ê´€ë ¨ ì¶”ê°€ ì´ˆê¸°í™” ===
        self.openset_enabled = hasattr(config, 'openset') and config.openset.enabled
        
        if self.openset_enabled:
            self.openset_config = config.openset
            
            # ğŸ ìƒˆë¡œìš´ ThresholdCalibrator ì´ˆê¸°í™” (FAR ëª¨ë“œ ì§€ì›)
            self.threshold_calibrator = ThresholdCalibrator(
                mode="cosine",
                threshold_mode=config.openset.threshold_mode,        # ğŸ 'far' or 'eer'
                target_far=config.openset.target_far,                # ğŸ 0.01 (1%)
                alpha=config.openset.threshold_alpha,                # ğŸ íŒŒë¼ë¯¸í„°ëª… í†µì¼
                max_delta=config.openset.threshold_max_delta,        # ğŸ íŒŒë¼ë¯¸í„°ëª… í†µì¼
                clip_range=(-1.0, 1.0),
                use_auto_margin=config.openset.use_margin,
                margin_init=config.openset.margin_tau,
                far_target=None,  # ğŸ auto-marginìš© (ë³„ë„)
                min_samples=10,
                verbose=config.openset.verbose_calibration           # ğŸ ìƒì„¸ ì¶œë ¥ ì˜µì…˜
            )
            
            # Dev/Train ë°ì´í„° ê´€ë¦¬
            self.dev_data = {}    # {user_id: (paths, labels)}
            self.train_data = {}  # {user_id: (paths, labels)}
            self.registered_users = set()
            
            # í‰ê°€ íˆìŠ¤í† ë¦¬
            self.evaluation_history = []
            
            # ì´ˆê¸° ì„ê³„ì¹˜ ì„¤ì •
            self.ncm.set_thresholds(
                tau_s=config.openset.initial_tau,
                use_margin=config.openset.use_margin,
                tau_m=config.openset.margin_tau
            )
            
            # ğŸ ëª¨ë“œë³„ ì´ˆê¸°ê°’ ì¡°ì •
            if config.openset.threshold_mode == 'far':
                print("ğŸ FAR Target mode enabled")
                print(f"   Target FAR: {config.openset.target_far*100:.1f}%")
            else:
                print("ğŸ‹ EER mode enabled")
            
            print(f"   Initial Ï„_s: {config.openset.initial_tau}")
            print(f"   Margin: {config.openset.use_margin} (Ï„_m={config.openset.margin_tau})")
        else:
            self.registered_users = set()
            print("ğŸ“Œ Open-set mode disabled")
        
        # Statistics
        self.experience_count = 0
        
        # ğŸŒ½ BASE_ID ì €ì¥ (ë„¤ê±°í‹°ë¸Œ í•„í„°ë§ìš©)
        self.base_id = int(config.negative.base_id) if hasattr(config, 'negative') else 10000
        print(f"ğŸŒ½ SCRTrainer using BASE_ID: {self.base_id}")
        
        # ğŸŒ½ ì›Œë°ì—… ê´€ë ¨ íŒŒë¼ë¯¸í„° ìºì‹±
        self.warmup_T = int(config.negative.warmup_experiences) if hasattr(config, 'negative') else 4
        self.r0 = float(config.negative.r0) if hasattr(config, 'negative') else 0.5
        self.max_neg_per_class = int(config.negative.max_per_batch) if hasattr(config, 'negative') else 1
        
        # ğŸŒˆ CuDNN ìµœì í™” (ê³ ì • í¬ê¸° ì…ë ¥)
        if torch.backends.cudnn.is_available():
            torch.backends.cudnn.benchmark = True
            torch.backends.cudnn.deterministic = False  # ğŸŒˆ ì†ë„ ìš°ì„  ëª…ì‹œ
            print("ğŸš€ CuDNN benchmark enabled for fixed-size inputs")
        
        # ì‚¬ì „í›ˆë ¨ ì‚¬ìš© ì—¬ë¶€ ë¡œê·¸
        if hasattr(config.model, 'use_pretrained') and config.model.use_pretrained:
            print(f"ğŸ”¥ SCRTrainer initialized with pretrained model")
        else:
            print(f"ğŸ² SCRTrainer initialized with random weights")
        
        # ğŸ’ ëª¨ë‹ˆí„°ë§ ë³€ìˆ˜ ì¶”ê°€
        self.proxy_loss_history = []
        self.drift_history = []

    # ğŸŒ½ í—¬í¼ ë©”ì„œë“œ ì¶”ê°€
    def _dedup_negative_classes(self, paths, labels, max_per_class=1):
        """ë„¤ê±°í‹°ë¸Œ í´ë˜ìŠ¤ ì¤‘ë³µ ì œê±°"""
        out_p, out_l, seen = [], [], set()
        for p, l in zip(paths, labels):
            li = int(l)
            if li >= self.base_id:
                if (li in seen) and (max_per_class == 1):
                    continue
                seen.add(li)
            out_p.append(p)
            out_l.append(l)
        return out_p, out_l
    
    def _memory_cap_for_ratio(self, num_current, r):
        """r0 ë¹„ìœ¨ ì œí•œ ê³„ì‚°"""
        if r <= 0:
            return 0
        return int((r / max(1e-8, 1.0 - r)) * num_current)
    
    # ğŸ’ Lambda ìŠ¤ì¼€ì¤„ë§ ë©”ì„œë“œ ì¶”ê°€
    def get_proxy_lambda(self):
        """Experience ê¸°ë°˜ lambda ìŠ¤ì¼€ì¤„ë§"""
        if not self.proxy_lambda_schedule or not self.use_proxy:
            return self.proxy_lambda  # ê³ ì •ê°’
        
        # ìŠ¤ì¼€ì¤„ ì ìš©
        lambda_val = 0.0
        
        if self.experience_count >= self.warmup_T:
            lambda_val = self.proxy_lambda_schedule.get('warmup', 0.1)
        if self.experience_count >= self.warmup_T + 10:
            lambda_val = self.proxy_lambda_schedule.get('warmup_10', 0.2)
        if self.experience_count >= self.warmup_T + 20:
            lambda_val = self.proxy_lambda_schedule.get('warmup_20', 0.3)
        
        return lambda_val
    
    # ğŸ’ í”„ë¡œí† íƒ€ì… ìºì‹œ ì—…ë°ì´íŠ¸ ë©”ì„œë“œ ì¶”ê°€
    @torch.no_grad()
    def update_proto_cache(self):
        """
        ğŸ’ 6144D NCM í”„ë¡œí† íƒ€ì…ì„ 128Dë¡œ íˆ¬ì˜í•˜ì—¬ ìºì‹œ
        Experience ëì— í•œ ë²ˆë§Œ ì‹¤í–‰
        """
        # Warmup ì²´í¬
        if self.experience_count < self.warmup_T:
            return
        
        # ì‹¤ì œ í´ë˜ìŠ¤ë§Œ (ë„¤ê±°í‹°ë¸Œ ì œì™¸)
        real_classes = [c for c in self.ncm.class_means_dict.keys()
                        if c < self.base_id]
        
        # ìµœì†Œ í´ë˜ìŠ¤ ìˆ˜ ì²´í¬
        if len(real_classes) < self.proxy_min_classes:
            print(f"â³ Waiting for more classes: {len(real_classes)}/{self.proxy_min_classes}")
            return
        
        # ProxyLoss í™œì„±í™”
        self.proxy_active = True
        
        # ì •ë ¬ëœ ìˆœì„œ ìœ ì§€ (ì¤‘ìš”!)
        self.proto_cache_ids = sorted(real_classes)
        
        # 6144D í”„ë¡œí† íƒ€ì… ìˆ˜ì§‘
        protos_6144 = torch.stack([
            self.ncm.class_means_dict[c]
            for c in self.proto_cache_ids
        ]).to(self.device)
        
        # 128Dë¡œ íˆ¬ì˜
        self.model.eval()
        protos_6144_norm = F.normalize(protos_6144, dim=-1)
        P = self.model.projection_head(protos_6144_norm)
        self.proto_cache_P = F.normalize(P, dim=-1).detach()  # ğŸ’ detach í•„ìˆ˜!
        self.model.train()
        
        print(f"ğŸ’ Proto cache updated: {len(self.proto_cache_ids)} classes")
        print(f"   Cache shape: {self.proto_cache_P.shape}")
    
    def train_experience(self, user_id: int, image_paths: List[str], labels: List[int]) -> Dict:
        """í•˜ë‚˜ì˜ experience (í•œ ëª…ì˜ ì‚¬ìš©ì) í•™ìŠµ - ì˜¤í”ˆì…‹ ì§€ì›."""
        
        print(f"\n=== Training Experience {self.experience_count}: User {user_id} ===")
        
        # ğŸ’ Lambda ìŠ¤ì¼€ì¤„ ì—…ë°ì´íŠ¸
        if self.use_proxy and self.proxy_loss:
            new_lambda = self.get_proxy_lambda()
            self.proxy_loss.lambda_proxy = new_lambda
            if self.experience_count % 10 == 0:
                print(f"ğŸ’ Lambda schedule: {new_lambda:.2f}")
        
        # ğŸ’ ë©”ëª¨ë¦¬ ë²„í¼ì— ê²½í—˜ ì¹´ìš´í„° ì „ë‹¬ (ë¡œê·¸ìš©)
        if hasattr(self.memory_buffer, 'set_experience_count'):
            self.memory_buffer.set_experience_count(self.experience_count)
        
        # ì›ë³¸ labelsë¥¼ ë³´ì¡´ (ì¤‘ìš”!)
        original_labels = labels.copy()
        
        # ğŸ‹ Step 1: Dev/Train ë¶„ë¦¬ (ì˜¤í”ˆì…‹ ëª¨ë“œì¸ ê²½ìš°)
        if self.openset_enabled:
            train_paths, train_labels, dev_paths, dev_labels = split_user_data(
                image_paths, original_labels,
                dev_ratio=self.openset_config.dev_ratio,
                min_dev=2
            )
            
            # ì €ì¥
            self.dev_data[user_id] = (dev_paths, dev_labels)
            self.train_data[user_id] = (train_paths, train_labels)
            
            print(f"ğŸ‹ Data split: Train={len(train_paths)}, Dev={len(dev_paths)}")
        else:
            train_paths = image_paths
            train_labels = original_labels
        
        self.registered_users.add(user_id)
        
        # í˜„ì¬ ì‚¬ìš©ì ë°ì´í„°ì…‹ ìƒì„± (Train ë°ì´í„°ë§Œ)
        current_dataset = MemoryDataset(
            paths=train_paths,
            labels=train_labels,
            transform=self.train_transform,
            train=True  # ğŸŒˆ ìë™ìœ¼ë¡œ dual_views=True (2ë·° ìƒì„±)
        )
        
        # í•™ìŠµ í†µê³„
        loss_avg = AverageMeter('Loss')
        
        # SCR ë…¼ë¬¸ ë°©ì‹: epochë‹¹ ì—¬ëŸ¬ iteration
        self.model.train()
        
        for epoch in range(self.config.training.scr_epochs):
            epoch_loss = 0
            
            for iteration in range(self.config.training.iterations_per_epoch):
                
                # 1. í˜„ì¬ ë°ì´í„°ì—ì„œ B_n ìƒ˜í”Œë§ (ì¤‘ë³µ ì œê±° + ì•ˆì „í•œ í¬ê¸°)
                current_indices = np.random.choice(
                    len(current_dataset),
                    size=min(len(current_dataset), self.config.training.scr_batch_size),
                    replace=False  # ì¤‘ë³µ ì œê±°!
                )
                current_subset = Subset(current_dataset, current_indices)
                
                # 2. ë©”ëª¨ë¦¬ì—ì„œ B_M ìƒ˜í”Œë§ (SCR ë…¼ë¬¸: ë§¤ iteration ìƒ˜í”Œë§)
                if len(self.memory_buffer) > 0:
                    # ğŸ’ ì»¤ë²„ë¦¬ì§€ ìƒ˜í”Œë§ ì‚¬ìš©
                    if self.use_coverage and hasattr(self.memory_buffer, 'coverage_sample'):
                        memory_paths, memory_labels = self.memory_buffer.coverage_sample(
                            self.config.training.memory_batch_size,
                            k_per_class=self.coverage_k
                        )
                        if iteration == 0 and epoch == 0:
                            print(f"ğŸ’ Using coverage sampling")
                    else:
                        # ğŸ§Š ê¸°ì¡´ ìƒ˜í”Œë§
                        memory_paths, memory_labels = self.memory_buffer.sample(
                            self.config.training.memory_batch_size
                        )
                    
                    if torch.is_tensor(memory_labels):
                        memory_labels = memory_labels.cpu().tolist()
                    
                    # ğŸŒ½ === ì›Œë°ì—… ê·œì¹™ ì ìš© ì‹œì‘ ===
                    if self.experience_count < self.warmup_T:
                        # (1) ë„¤ê±°í‹°ë¸Œ ì¤‘ë³µ ê¸ˆì§€ (ë™ì¼ neg-class 2ì¥ ê¸ˆì§€)
                        memory_paths, memory_labels = self._dedup_negative_classes(
                            memory_paths, memory_labels, max_per_class=self.max_neg_per_class
                        )
                        
                        # (2) r0 ë¹„ìœ¨ ì œí•œ: mem â‰¤ floor(r0/(1-r0) * len(current_subset))
                        cap = self._memory_cap_for_ratio(len(current_indices), self.r0)
                        if cap >= 0 and len(memory_paths) > cap:
                            idx = np.random.choice(len(memory_paths), size=cap, replace=False)
                            memory_paths = [memory_paths[i] for i in idx]
                            memory_labels = [memory_labels[i] for i in idx]
                    else:
                        # ì›Œë°ì—… ì¢…ë£Œ ì´í›„: ë„¤ê±°í‹°ë¸Œ ì™„ì „ ì œì™¸ (purge ì•ˆì „ì¥ì¹˜)
                        kept = [(p, l) for p, l in zip(memory_paths, memory_labels) if int(l) < self.base_id]
                        memory_paths, memory_labels = (list(map(list, zip(*kept))) if kept else ([], []))
                    # ğŸŒ½ === ì›Œë°ì—… ê·œì¹™ ì ìš© ë ===
                    
                    # ğŸŒ½ (ë””ë²„ê·¸) í˜„ì¬ ë©”ëª¨ë¦¬ ë°°ì¹˜ ì•ˆ ë„¤ê±°í‹°ë¸Œ ê°œìˆ˜
                    neg_in_mem = sum(1 for l in memory_labels if int(l) >= self.base_id)
                    if iteration == 0 and epoch == 0:
                        print(f"[DEBUG][exp{self.experience_count}] cur={len(current_indices)}, mem={len(memory_paths)}, neg_in_mem={neg_in_mem}, r0={self.r0}")
                    
                    # ğŸŒ½ ë©”ëª¨ë¦¬ê°€ ìˆì„ ë•Œë§Œ ê²°í•©
                    if memory_paths:
                        # ë©”ëª¨ë¦¬ ë°ì´í„°ì…‹ ìƒì„±
                        memory_dataset = MemoryDataset(
                            paths=memory_paths,
                            labels=memory_labels,
                            transform=self.train_transform,
                            train=True  # ğŸŒˆ í•™ìŠµìš©ì´ë¯€ë¡œ 2ë·° ìœ ì§€
                        )
                        
                        # 3. ConcatDatasetìœ¼ë¡œ ê²°í•©
                        combined_dataset = ConcatDataset([current_subset, memory_dataset])
                    else:
                        combined_dataset = current_subset
                        if self.experience_count == self.warmup_T:
                            print("ğŸ“Œ First post-warmup experience: memory empty, using current only")
                else:
                    combined_dataset = current_subset
                
                # 4. DataLoaderë¡œ ë°°ì¹˜ ìƒì„± (ğŸŒˆ ìµœì í™”)
                batch_loader = DataLoader(
                    combined_dataset,
                    batch_size=len(combined_dataset),
                    shuffle=False,  # ìˆœì„œ ìœ ì§€ ì¤‘ìš”!
                    num_workers=0,  # ğŸŒˆ ë§¤ë²ˆ ìƒì„±ì´ë¯€ë¡œ 0 ìœ ì§€
                    pin_memory=True,  # ğŸŒˆ GPU ì „ì†¡ ìµœì í™”
                    persistent_workers=False  # ğŸŒˆ ëª…ì‹œì  False
                )
                
                # 5. í•™ìŠµ
                for data, batch_labels in batch_loader:
                    batch_size = len(batch_labels)
                    
                    # ì˜¬ë°”ë¥¸ view ë°°ì¹˜
                    view1 = data[0]  # ì²« ë²ˆì§¸ ì¦ê°•ë“¤
                    view2 = data[1]  # ë‘ ë²ˆì§¸ ì¦ê°•ë“¤
                    
                    # ì˜¬ë°”ë¥¸ ìˆœì„œë¡œ ì—°ê²° (ğŸŒˆ non_blocking ì¶”ê°€)
                    x = torch.cat([view1, view2], dim=0).to(self.device, non_blocking=True)
                    batch_labels = batch_labels.to(self.device, non_blocking=True)
                    
                    # Forward
                    self.optimizer.zero_grad()
                    # ğŸ§Š features = self.model(x)  # [2*batch_size, feature_dim]
                    features_all = self.model(x)  # ğŸ’ ë³€ìˆ˜ëª… ë³€ê²½ [2*batch_size, 128]
                    
                    # ì˜¬ë°”ë¥¸ í˜ì–´ë§
                    f1 = features_all[:batch_size]  # ğŸ’
                    f2 = features_all[batch_size:]   # ğŸ’
                    
                    # ğŸŒ½ ì²« ë°°ì¹˜ ë””ë²„ê·¸ ì¶œë ¥
                    if iteration == 0 and epoch == 0:
                        neg_in_batch = int((batch_labels >= self.base_id).sum().item())
                        real_in_batch = int(len(batch_labels) - neg_in_batch)
                        print(f"[DEBUG][exp{self.experience_count}] batch real={real_in_batch}, neg={neg_in_batch}")
                        
                        with torch.no_grad():
                            # Positive pairs: f1[i]ì™€ f2[i]
                            pos_sim = torch.nn.functional.cosine_similarity(f1, f2, dim=1).mean()
                            # Negative pairs: f1[i]ì™€ f2[i+1]
                            neg_sim = torch.nn.functional.cosine_similarity(
                                f1, f2.roll(shifts=1, dims=0), dim=1
                            ).mean()
                            print(f"  ğŸ“Š Similarity check:")
                            print(f"     Positive pairs: {pos_sim:.4f} (should be high)")
                            print(f"     Negative pairs: {neg_sim:.4f} (should be low)")
                            if pos_sim <= neg_sim:
                                print("  âš ï¸  WARNING: Positive pairs not more similar! Check pairing!")
                    
                    # SupConLoss í˜•ì‹ìœ¼ë¡œ reshape: [batch_size, 2, feature_dim]
                    # ğŸ§Š features = torch.stack([f1, f2], dim=1)
                    features_paired = torch.stack([f1, f2], dim=1)  # ğŸ’ ìƒˆ ë³€ìˆ˜ëª…
                    
                    # ì°¨ì› í™•ì¸
                    assert features_paired.shape[0] == batch_labels.shape[0], \
                        f"Batch size mismatch: features {features_paired.shape[0]} vs labels {batch_labels.shape[0]}"
                    assert features_paired.shape[1] == 2, \
                        f"Must have 2 views, got {features_paired.shape[1]}"
                    
                    # Calculate loss
                    # ğŸ§Š loss = self.criterion(features, batch_labels)
                    loss = self.criterion(features_paired, batch_labels)  # ğŸ’ SupConLoss
                    
                    # ğŸ’ ProxyContrastLoss ì¶”ê°€
                    if self.proxy_active and self.proto_cache_P is not None and self.use_proxy:
                        # ì •ê·œí™” (í•„ìˆ˜!)
                        z_all = F.normalize(features_all, dim=-1)
                        y_all = batch_labels.repeat(2).long()  # ë‘ ë·° ë¼ë²¨
                        
                        loss_proxy = self.proxy_loss(
                            z_all, y_all,
                            self.proto_cache_P,
                            self.proto_cache_ids
                        )
                        
                        loss = loss + loss_proxy
                        
                        # ëª¨ë‹ˆí„°ë§
                        if iteration == 0 and epoch == 0:
                            print(f"ğŸ’ ProxyLoss: {loss_proxy.item():.4f}, SupCon: {loss.item() - loss_proxy.item():.4f}")
                            self.proxy_loss_history.append(loss_proxy.item())
                    
                    loss_avg.update(loss.item(), batch_size)
                    
                    # Backward with gradient clipping
                    loss.backward()
                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
                    self.optimizer.step()
                    
                    epoch_loss += loss.item()
            
            if (epoch + 1) % 5 == 0:
                avg_loss = epoch_loss / self.config.training.iterations_per_epoch
                print(f"  Epoch [{epoch+1}/{self.config.training.scr_epochs}] Loss: {avg_loss:.4f}")

        # ğŸŒˆ ì˜¬ë°”ë¥¸ ìˆœì„œ: ë²„í¼ ë¨¼ì €, NCM ë‚˜ì¤‘!
        print(f"\n=== Before buffer update ===")
        print(f"Buffer size: {len(self.memory_buffer)}")
        print(f"Buffer seen classes: {self.memory_buffer.seen_classes if hasattr(self.memory_buffer, 'seen_classes') else 'N/A'}")
        
        # Step 1: ë©”ëª¨ë¦¬ ë²„í¼ ì—…ë°ì´íŠ¸ (Train ë°ì´í„°ë§Œ!)
        self.memory_buffer.update_from_dataset(train_paths, train_labels)
        self._full_memory_dataset = None  # ìºì‹œ ë¬´íš¨í™”
        print(f"Memory buffer size after update: {len(self.memory_buffer)}")
        
        # Step 2: NCM ì—…ë°ì´íŠ¸ (ë²„í¼ ì—…ë°ì´íŠ¸ í›„!)
        self._update_ncm()
        
        # ğŸ’ Step 3: í”„ë¡œí† íƒ€ì… ìºì‹œ ì—…ë°ì´íŠ¸ (Experience ë!)
        if self.use_proxy:
            self.update_proto_cache()
        
        # ë””ë²„ê¹…: NCMê³¼ ë²„í¼ ë™ê¸°í™” í™•ì¸
        all_paths, all_labels = self.memory_buffer.get_all_data()
        buffer_classes = set(int(label) for label in all_labels)
        ncm_classes = set(self.ncm.class_means_dict.keys())
        missing = buffer_classes - ncm_classes
        
        if missing:
            print(f"âš ï¸  NCM missing classes: {sorted(list(missing))}")
        else:
            print(f"âœ… NCM synchronized: {len(ncm_classes)} classes")
        
        # ğŸ‹ Step 4: ì£¼ê¸°ì  ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ë° í‰ê°€ (ì˜¤í”ˆì…‹ ëª¨ë“œ)
        if self.openset_enabled and len(self.registered_users) % self.config.training.test_interval == 0:
            
            # Warmup ì²´í¬
            if len(self.registered_users) >= self.openset_config.warmup_users:
                print("\n" + "="*60)
                print("ğŸ”„ THRESHOLD CALIBRATION & EVALUATION")
                print("="*60)
                
                # ìº˜ë¦¬ë¸Œë ˆì´ì…˜
                self._calibrate_threshold()
                
                # í‰ê°€
                metrics = self._evaluate_openset()
                
                # ê²°ê³¼ ì €ì¥
                self.evaluation_history.append({
                    'num_users': len(self.registered_users),
                    'tau_s': self.ncm.tau_s,
                    'tau_m': self.ncm.tau_m if self.ncm.use_margin else None,
                    'metrics': metrics
                })
                
                print("="*60 + "\n")
            else:
                print(f"ğŸ“Œ Warmup phase: {len(self.registered_users)}/{self.openset_config.warmup_users}")
        
        # 7. Experience ì¹´ìš´í„° ì¦ê°€
        self.experience_count += 1
        
        # 8. Scheduler step
        self.scheduler.step()
        
        # ğŸŒ½ ë°˜í™˜ê°’ì— neg_in_batch ì¶”ê°€
        return {
            'experience': self.experience_count - 1,
            'user_id': user_id,
            'loss': loss_avg.avg,
            'memory_size': len(self.memory_buffer),
            'num_registered': len(self.registered_users),
            'neg_in_batch': neg_in_mem if 'neg_in_mem' in locals() else 0  # ğŸŒ½
        }
    
    # ğŸ‹ === ì˜¤í”ˆì…‹ ê´€ë ¨ ìƒˆ ë©”ì„œë“œë“¤ ì¶”ê°€ ===
    @torch.no_grad()
    def _calibrate_threshold(self):
        """ğŸ‹ EER ê¸°ë°˜ ì„ê³„ì¹˜ ìº˜ë¦¬ë¸Œë ˆì´ì…˜"""
        
        # ğŸ ëª¨ë“œ í‘œì‹œ ì¶”ê°€
        print(f"\nğŸ“Š Extracting scores for {self.openset_config.threshold_mode.upper()} calibration...")
        
        # Dev ë°ì´í„° ìˆ˜ì§‘
        all_dev_paths = []
        all_dev_labels = []
        for uid, (paths, labels) in self.dev_data.items():
            all_dev_paths.extend(paths)
            all_dev_labels.extend([uid] * len(paths))
        
        # ì ìˆ˜ ì¶”ì¶œ
        print("ğŸ“Š Extracting scores...")
        
        # Genuine scores
        s_genuine = extract_scores_genuine(
            self.model, self.ncm,
            all_dev_paths, all_dev_labels,
            self.test_transform, self.device
        )
        
        # 2-1. Between impostor (ë“±ë¡ìë¼ë¦¬ êµì°¨)
        s_imp_between = extract_scores_impostor_between(
            self.model, self.ncm,
            all_dev_paths, all_dev_labels,
            self.test_transform, self.device,
            max_pairs=2000
        )
        
        # 2-2. NegRef impostor (ì™¸ë¶€ ë°ì´í„°)
        s_imp_negref = extract_scores_impostor_negref(
            self.model, self.ncm,
            self.config.dataset.negative_samples_file,
            self.test_transform, self.device,
            max_eval=self.openset_config.negref_max_eval
        )
        
        # ğŸ ê· í˜• ë§ì¶”ê¸° (Between + NegRefë§Œ!)
        # Unknown ì œê±° - í‰ê°€ìš©ì´ë¯€ë¡œ
        s_impostor = balance_impostor_scores(
            s_imp_between,
            None,  # Unknown ì•ˆ ì”€!
            s_imp_negref,
            ratio=(0.3, 0.0, 0.7)  # Between 30%, NegRef 70%
        )
        
        print(f"   Genuine: {len(s_genuine)} pairs")
        print(f"   Impostor: {len(s_impostor)} pairs (Between + NegRef)")
        
        # ğŸ ìº˜ë¦¬ë¸Œë ˆì´ì…˜ (EER ë˜ëŠ” FAR ëª¨ë“œ)
        if len(s_genuine) >= 10 and len(s_impostor) >= 10:
            # calibrate ë©”ì„œë“œê°€ ë‚´ë¶€ì ìœ¼ë¡œ EER/FAR ê³„ì‚°í•¨!
            result = self.threshold_calibrator.calibrate(
                genuine_scores=s_genuine,
                impostor_scores=s_impostor,
                old_tau=self.ncm.tau_s
            )
            
            # NCMì— ì ìš©
            self.ncm.set_thresholds(
                tau_s=result['tau_smoothed'],
                use_margin=self.openset_config.use_margin,
                tau_m=self.openset_config.margin_tau
            )
            
            # ğŸ ëª¨ë“œë³„ ì¶œë ¥
            if self.openset_config.threshold_mode == 'eer':
                print(f"ğŸ“Š EER Mode Results:")
                print(f"   EER: {result.get('eer', 0)*100:.2f}%")
                print(f"   Threshold: {result['tau_smoothed']:.4f}")
            else:  # far mode
                print(f"ğŸ FAR Target Results:")
                print(f"   Target FAR: {self.openset_config.target_far*100:.1f}%")
                print(f"   Achieved FAR: {result.get('current_far', 0)*100:.1f}%")
                print(f"   Threshold: {result['tau_smoothed']:.4f}")
        else:
            print("âš ï¸ Not enough samples for calibration")
    
    @torch.no_grad()
    def _evaluate_openset(self):
        """ğŸ‹ ì˜¤í”ˆì…‹ í‰ê°€"""
        
        print("\nğŸ“ˆ Open-set Evaluation:")
        
        # 1. Known-Dev (TAR/FRR)
        all_dev_paths = []
        all_dev_labels = []
        for uid, (paths, labels) in self.dev_data.items():
            all_dev_paths.extend(paths)
            all_dev_labels.extend([uid] * len(paths))
        
        if all_dev_paths:
            preds = predict_batch(
                self.model, self.ncm,
                all_dev_paths, self.test_transform, self.device
            )
            
            correct = sum(1 for p, l in zip(preds, all_dev_labels) if p == l)
            rejected = sum(1 for p in preds if p == -1)
            
            TAR = correct / max(1, len(preds))
            FRR = rejected / max(1, len(preds))
        else:
            TAR = FRR = 0.0
        
        # 2. Unknown (TRR/FAR)
        unknown_paths, unknown_labels = load_paths_labels_from_txt(
            self.config.dataset.train_set_file
        )
        
        # ë¯¸ë“±ë¡ ì‚¬ìš©ìë§Œ í•„í„°ë§
        unknown_filtered = []
        for p, l in zip(unknown_paths, unknown_labels):
            if l not in self.registered_users:
                unknown_filtered.append(p)
        
        # ìƒ˜í”Œë§
        if len(unknown_filtered) > 1000:
            unknown_filtered = np.random.choice(unknown_filtered, 1000, replace=False).tolist()
        
        if unknown_filtered:
            preds_unk = predict_batch(
                self.model, self.ncm,
                unknown_filtered, self.test_transform, self.device
            )
            TRR_u = sum(1 for p in preds_unk if p == -1) / len(preds_unk)
            FAR_u = 1 - TRR_u
        else:
            TRR_u = FAR_u = 0.0
        
        # 3. NegRef (ì„ íƒ)
        negref_paths, _ = load_paths_labels_from_txt(
            self.config.dataset.negative_samples_file
        )
        
        if len(negref_paths) > 1000:
            negref_paths = np.random.choice(negref_paths, 1000, replace=False).tolist()
        
        if negref_paths:
            preds_neg = predict_batch(
                self.model, self.ncm,
                negref_paths, self.test_transform, self.device
            )
            TRR_n = sum(1 for p in preds_neg if p == -1) / len(preds_neg)
            FAR_n = 1 - TRR_n
        else:
            TRR_n = FAR_n = None
        
        # ê²°ê³¼ ì¶œë ¥
        print(f"   Known-Dev: TAR={TAR:.3f}, FRR={FRR:.3f}")
        print(f"   Unknown: TRR={TRR_u:.3f}, FAR={FAR_u:.3f}")
        if TRR_n is not None:
            print(f"   NegRef: TRR={TRR_n:.3f}, FAR={FAR_n:.3f}")
        print(f"   Threshold: Ï„_s={self.ncm.tau_s:.4f}")
        
        # ğŸ ëª¨ë“œ ì •ë³´ ì¶”ê°€
        if self.openset_config.threshold_mode == 'far':
            print(f"   Mode: FAR Target ({self.openset_config.target_far*100:.1f}%)")
        else:
            print(f"   Mode: EER")
        
        return {
            'TAR': TAR, 'FRR': FRR,
            'TRR_unknown': TRR_u, 'FAR_unknown': FAR_u,
            'TRR_negref': TRR_n, 'FAR_negref': FAR_n,
            'mode': self.openset_config.threshold_mode  # ğŸ ëª¨ë“œ ì •ë³´ ì¶”ê°€
        }
    
    @torch.no_grad()
    def _update_ncm(self):
        """
        NCM classifierì˜ class meansë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.
        ğŸ„ ë©”ëª¨ë¦¬ ë²„í¼ì˜ ë°ì´í„° ì¤‘ ì‹¤ì œ ìœ ì €ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤ (ê°€ì§œ í´ë˜ìŠ¤ ì œì™¸).
        """
        if len(self.memory_buffer) == 0:
            return
            
        self.model.eval()
        
        # ë©”ëª¨ë¦¬ ë²„í¼ì—ì„œ ëª¨ë“  ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
        all_paths, all_labels = self.memory_buffer.get_all_data()
        
        # ğŸ„ ê°€ì§œ í´ë˜ìŠ¤ í•„í„°ë§
        real_paths = []
        real_labels = []
        fake_count = 0
        
        for path, label in zip(all_paths, all_labels):
            label_int = int(label) if not isinstance(label, int) else label
            if label_int < self.base_id:  # ğŸŒ½ ë™ì  base_id ì‚¬ìš©
                real_paths.append(path)
                real_labels.append(label)
            else:
                fake_count += 1
        
        # ğŸ„ ì‹¤ì œ ìœ ì €ê°€ ì—†ìœ¼ë©´ NCM ë¹„ì›Œë‘ 
        if not real_paths:
            print("âš ï¸ No real users for NCM update (NCM remains empty)")
            return
        
        # ğŸ„ í•„í„°ë§ ê²°ê³¼ ì¶œë ¥
        if fake_count > 0:
            print(f"ğŸ„ NCM update: {len(real_paths)} real samples ({fake_count} fake samples filtered out)")
        
        # ğŸ„ ì‹¤ì œ ìœ ì € ë°ì´í„°ë¡œë§Œ ë°ì´í„°ì…‹ ìƒì„±
        dataset = MemoryDataset(
            paths=real_paths,  # ê°€ì§œ ì œì™¸ëœ ê²½ë¡œ
            labels=real_labels,  # ê°€ì§œ ì œì™¸ëœ ë¼ë²¨
            transform=self.test_transform,
            train=False  # ìë™ìœ¼ë¡œ dual_views=False (1ë·°ë§Œ ìƒì„±)
        )
        
        # ğŸŒˆ DataLoader ìµœì í™”
        dataloader = DataLoader(
            dataset,
            batch_size=128,
            shuffle=False,
            num_workers=self.config.training.num_workers,
            pin_memory=True,  # ğŸŒˆ GPU ì „ì†¡ ìµœì í™”
            persistent_workers=False  # ğŸŒˆ ë‹¨ìˆœí™”
        )
        
        # í´ë˜ìŠ¤ë³„ë¡œ features ìˆ˜ì§‘
        class_features = {}
        
        for data, labels in dataloader:
            # ğŸŒˆ ì•ˆì „ì¥ì¹˜ + non_blocking
            data = _ensure_single_view(data).to(self.device, non_blocking=True)
            labels = labels.to(self.device, non_blocking=True)
            
            # Feature extraction
            features = self.model.getFeatureCode(data)
            
            # í´ë˜ìŠ¤ë³„ë¡œ ë¶„ë¥˜
            for i, label in enumerate(labels):
                label_item = label.item()
                if label_item not in class_features:
                    class_features[label_item] = []
                class_features[label_item].append(features[i].cpu())
        
        # í´ë˜ìŠ¤ë³„ í‰ê·  ê³„ì‚°
        class_means = {}
        for label, features_list in class_features.items():
            if len(features_list) > 0:
                mean_feature = torch.stack(features_list).mean(dim=0)
                # ğŸŒˆ eps ì¶”ê°€ (NaN ë°©ì§€)
                mean_feature = mean_feature / (mean_feature.norm(p=2) + 1e-12)
                class_means[label] = mean_feature
        
        # NCM ì—…ë°ì´íŠ¸ (ì™„ì „ êµì²´ ë°©ì‹)
        self.ncm.replace_class_means_dict(class_means)
        
        # ğŸ„ ì‹¤ì œ í´ë˜ìŠ¤ë§Œ í™•ì¸
        real_classes = [k for k in class_means.keys() if k < self.base_id]  # ğŸŒ½ ë™ì  base_id
        print(f"ğŸ„ Updated NCM with {len(real_classes)} real classes (no fake contamination)")
        
        self.model.train()
    
    def evaluate(self, test_dataset: Dataset) -> float:
        """
        NCMì„ ì‚¬ìš©í•˜ì—¬ ì •í™•ë„ë¥¼ í‰ê°€í•©ë‹ˆë‹¤.
        """
        self.model.eval()
        
        # ğŸŒˆ DataLoader ìµœì í™”
        dataloader = DataLoader(
            test_dataset,
            batch_size=128,
            shuffle=False,
            num_workers=self.config.training.num_workers,
            pin_memory=True,  # ğŸŒˆ GPU ì „ì†¡ ìµœì í™”
            persistent_workers=False  # ğŸŒˆ ë‹¨ìˆœí™”
        )
        
        correct = 0
        total = 0
        
        with torch.no_grad():
            for data, labels in dataloader:
                # ğŸŒˆ ì•ˆì „ì¥ì¹˜ + non_blocking
                data = _ensure_single_view(data).to(self.device, non_blocking=True)
                labels = labels.to(self.device, non_blocking=True)
                
                # Feature extraction
                features = self.model.getFeatureCode(data)
                
                # NCM prediction
                predictions = self.ncm.predict(features)
                
                correct += (predictions == labels).sum().item()
                total += labels.size(0)
        
        accuracy = 100.0 * correct / total
        return accuracy
    
    def save_checkpoint(self, path: str):
        """ì²´í¬í¬ì¸íŠ¸ ì €ì¥"""
        # ğŸ‹ ì˜¤í”ˆì…‹ ê´€ë ¨ ë°ì´í„°ë„ ì €ì¥
        checkpoint_dict = {
            'model_state_dict': self.model.state_dict(),
            'ncm_state_dict': self.ncm.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'experience_count': self.experience_count,
            'memory_buffer_size': len(self.memory_buffer)
        }
        
        # ğŸ’ ProxyLoss ê´€ë ¨ ì €ì¥
        if self.use_proxy:
            checkpoint_dict['proxy_data'] = {
                'proto_cache_P': self.proto_cache_P,
                'proto_cache_ids': self.proto_cache_ids,
                'proxy_active': self.proxy_active,
                'proxy_loss_history': self.proxy_loss_history
            }
        
        # ğŸ‹ ì˜¤í”ˆì…‹ ê´€ë ¨ ì¶”ê°€ ì €ì¥
        if self.openset_enabled:
            checkpoint_dict['openset_data'] = {
                'tau_s': self.ncm.tau_s,
                'tau_m': self.ncm.tau_m,
                'registered_users': list(self.registered_users),
                'evaluation_history': self.evaluation_history
            }
        
        torch.save(checkpoint_dict, path)
    
    def load_checkpoint(self, path: str):
        """ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ"""
        checkpoint = torch.load(path)
        self.model.load_state_dict(checkpoint['model_state_dict'])
        self.ncm.load_state_dict(checkpoint['ncm_state_dict'])
        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
        self.experience_count = checkpoint['experience_count']
        
        # ğŸ’ ProxyLoss ê´€ë ¨ ë³µì›
        if 'proxy_data' in checkpoint and self.use_proxy:
            proxy_data = checkpoint['proxy_data']
            self.proto_cache_P = proxy_data.get('proto_cache_P')
            self.proto_cache_ids = proxy_data.get('proto_cache_ids', [])
            self.proxy_active = proxy_data.get('proxy_active', False)
            self.proxy_loss_history = proxy_data.get('proxy_loss_history', [])
        
        # ğŸ‹ ì˜¤í”ˆì…‹ ê´€ë ¨ ë³µì›
        if 'openset_data' in checkpoint and self.openset_enabled:
            openset_data = checkpoint['openset_data']
            self.ncm.tau_s = openset_data.get('tau_s')
            self.ncm.tau_m = openset_data.get('tau_m')
            self.registered_users = set(openset_data.get('registered_users', []))
            self.evaluation_history = openset_data.get('evaluation_history', [])