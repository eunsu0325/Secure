import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset, Subset, ConcatDataset
from torch.optim import lr_scheduler
import numpy as np
from typing import Dict, List, Tuple, Optional
from tqdm import tqdm
from PIL import Image

from loss import SupConLoss
from models import get_scr_transforms
from utils.util import AverageMeter
from utils.pretrained_loader import PretrainedLoader


class MemoryDataset(Dataset):
    def __init__(self, paths: List[str], labels: List[int], transform, train=True):
        self.paths = paths
        
        # í…ì„œì¸ ê²½ìš° CPUë¡œ ì´ë™ í›„ numpy ë³€í™˜
        if torch.is_tensor(labels):
            self.labels = labels.cpu().numpy()
        else:
            self.labels = np.array(labels)
            
        self.transform = transform
        self.train = train
        
    def __len__(self):
        return len(self.paths)
    
    def __getitem__(self, index):
        path = self.paths[index]
        label = self.labels[index]
        
        # ì´ë¯¸ì§€ ë¡œë“œ
        img = Image.open(path).convert('L')
        
        # ê°™ì€ ì´ë¯¸ì§€ì— ë‹¤ë¥¸ augmentation ì ìš©
        data1 = self.transform(img)
        data2 = self.transform(img)
        
        return [data1, data2], label


class SCRTrainer:
    """
    Supervised Contrastive Replay Trainer.
    """
    
    def __init__(self, 
                 model: nn.Module,
                 ncm_classifier,
                 memory_buffer,
                 config,
                 device='cuda'):
        
        # ì‚¬ì „í›ˆë ¨ ë¡œë”© (ëª¨ë¸ì„ deviceë¡œ ì˜®ê¸°ê¸° ì „ì—)
        if hasattr(config.model, 'use_pretrained') and config.model.use_pretrained:
            if config.model.pretrained_path and config.model.pretrained_path.exists():
                print(f"\nğŸ“¦ Loading pretrained weights in SCRTrainer...")
                loader = PretrainedLoader()
                try:
                    model = loader.load_ccnet_pretrained(
                        model=model,
                        checkpoint_path=config.model.pretrained_path,
                        device=device,
                        verbose=True
                    )
                    print("âœ… Pretrained weights loaded successfully in trainer!")
                except Exception as e:
                    print(f"âš ï¸  Failed to load pretrained: {e}")
                    print("Continuing with current weights...")
            else:
                print(f"âš ï¸  Pretrained path not found: {config.model.pretrained_path}")
        
        self.model = model
        self.ncm = ncm_classifier
        self.memory_buffer = memory_buffer
        self.config = config
        self.device = device
        
        # Loss function
        self.criterion = SupConLoss(
            temperature=config.training.temperature,
            base_temperature=config.training.temperature
        )
        
        # Optimizer
        self.optimizer = optim.Adam(
            model.parameters(), 
            lr=config.training.learning_rate
        )
        
        # Scheduler
        self.scheduler = lr_scheduler.StepLR(
            self.optimizer,
            step_size=config.training.scheduler_step_size,
            gamma=config.training.scheduler_gamma
        )
        
        # Transform
        self.train_transform = get_scr_transforms(
            train=True,
            imside=config.dataset.height,
            channels=config.dataset.channels
        )
        
        self.test_transform = get_scr_transforms(
            train=False,
            imside=config.dataset.height,
            channels=config.dataset.channels
        )
        
        # ì „ì²´ ë©”ëª¨ë¦¬ ë°ì´í„°ì…‹ ìºì‹œ (íš¨ìœ¨ì„± ê°œì„ )
        self._full_memory_dataset = None
        self._memory_size_at_last_update = 0
        
        # Statistics
        self.experience_count = 0
        
        # ì‚¬ì „í›ˆë ¨ ì‚¬ìš© ì—¬ë¶€ ë¡œê·¸
        if hasattr(config.model, 'use_pretrained') and config.model.use_pretrained:
            print(f"ğŸ”¥ SCRTrainer initialized with pretrained model")
        else:
            print(f"ğŸ² SCRTrainer initialized with random weights")
    
    def train_experience(self, user_id: int, image_paths: List[str], labels: List[int]) -> Dict:
        """í•˜ë‚˜ì˜ experience (í•œ ëª…ì˜ ì‚¬ìš©ì) í•™ìŠµ."""
        
        print(f"\n=== Training Experience {self.experience_count}: User {user_id} ===")
        
        # ì›ë³¸ labelsë¥¼ ë³´ì¡´ (ì¤‘ìš”!)
        original_labels = labels.copy()
        
        # í˜„ì¬ ì‚¬ìš©ì ë°ì´í„°ì…‹ ìƒì„±
        current_dataset = MemoryDataset(
            paths=image_paths,
            labels=labels,
            transform=self.train_transform,
            train=True
        )
        
        # í•™ìŠµ í†µê³„
        loss_avg = AverageMeter('Loss')
        
        # SCR ë…¼ë¬¸ ë°©ì‹: epochë‹¹ ì—¬ëŸ¬ iteration
        self.model.train()
        
        for epoch in range(self.config.training.scr_epochs):
            epoch_loss = 0
            
            for iteration in range(self.config.training.iterations_per_epoch):
                
                # 1. í˜„ì¬ ë°ì´í„°ì—ì„œ B_n ìƒ˜í”Œë§ (ì¤‘ë³µ ì œê±° + ì•ˆì „í•œ í¬ê¸°)
                current_indices = np.random.choice(
                    len(current_dataset), 
                    size=min(len(current_dataset), self.config.training.scr_batch_size),
                    replace=False  # ì¤‘ë³µ ì œê±°!
                )
                current_subset = Subset(current_dataset, current_indices)
                
                # 2. ë©”ëª¨ë¦¬ì—ì„œ B_M ìƒ˜í”Œë§
                if len(self.memory_buffer) > 0:
                    # ë©”ëª¨ë¦¬ì—ì„œ ìƒ˜í”Œë§
                    memory_paths, memory_labels = self.memory_buffer.sample(
                        self.config.training.memory_batch_size
                    )
                    
                    if torch.is_tensor(memory_labels):
                        memory_labels = memory_labels.cpu().tolist()

                    # ë©”ëª¨ë¦¬ ë°ì´í„°ì…‹ ìƒì„±
                    memory_dataset = MemoryDataset(
                        paths=memory_paths,
                        labels=memory_labels,
                        transform=self.train_transform,
                        train=True
                    )
                    
                    # 3. ConcatDatasetìœ¼ë¡œ ê²°í•©
                    combined_dataset = ConcatDataset([current_subset, memory_dataset])
                else:
                    combined_dataset = current_subset
                
                # 4. DataLoaderë¡œ ë°°ì¹˜ ìƒì„±
                batch_loader = DataLoader(
                    combined_dataset,
                    batch_size=len(combined_dataset),
                    shuffle=False,  # ìˆœì„œ ìœ ì§€ ì¤‘ìš”!
                    num_workers=0
                )
                
                # 5. í•™ìŠµ
                for data, batch_labels in batch_loader:
                    batch_size = len(batch_labels)
                    
                    # ì˜¬ë°”ë¥¸ view ë°°ì¹˜
                    view1 = data[0]  # ì²« ë²ˆì§¸ ì¦ê°•ë“¤
                    view2 = data[1]  # ë‘ ë²ˆì§¸ ì¦ê°•ë“¤
                    
                    # ì˜¬ë°”ë¥¸ ìˆœì„œë¡œ ì—°ê²°
                    x = torch.cat([view1, view2], dim=0).to(self.device)
                    batch_labels = batch_labels.to(self.device)
                    
                    # Forward
                    self.optimizer.zero_grad()
                    features = self.model(x)  # [2*batch_size, feature_dim]
                    
                    # ì˜¬ë°”ë¥¸ í˜ì–´ë§
                    f1 = features[:batch_size]
                    f2 = features[batch_size:]
                    
                    # ê²€ì¦: ì²« ë²ˆì§¸ iterationì—ì„œ positive similarity í™•ì¸
                    if iteration == 0 and epoch == 0:
                        with torch.no_grad():
                            # Positive pairs: f1[i]ì™€ f2[i]
                            pos_sim = torch.nn.functional.cosine_similarity(f1, f2, dim=1).mean()
                            # Negative pairs: f1[i]ì™€ f2[i+1]
                            neg_sim = torch.nn.functional.cosine_similarity(
                                f1, f2.roll(shifts=1, dims=0), dim=1
                            ).mean()
                            print(f"  ğŸ“Š Similarity check:")
                            print(f"     Positive pairs: {pos_sim:.4f} (should be high)")
                            print(f"     Negative pairs: {neg_sim:.4f} (should be low)")
                            if pos_sim <= neg_sim:
                                print("  âš ï¸  WARNING: Positive pairs not more similar! Check pairing!")
                    
                    # SupConLoss í˜•ì‹ìœ¼ë¡œ reshape: [batch_size, 2, feature_dim]
                    features = torch.stack([f1, f2], dim=1)
                    
                    # ì°¨ì› í™•ì¸
                    assert features.shape[0] == batch_labels.shape[0], \
                        f"Batch size mismatch: features {features.shape[0]} vs labels {batch_labels.shape[0]}"
                    assert features.shape[1] == 2, \
                        f"Must have 2 views, got {features.shape[1]}"
                    
                    # Calculate loss
                    loss = self.criterion(features, batch_labels)
                    loss_avg.update(loss.item(), batch_size)
                    
                    # Backward with gradient clipping
                    loss.backward()
                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
                    self.optimizer.step()
                    
                    epoch_loss += loss.item()
            
            if (epoch + 1) % 5 == 0:
                avg_loss = epoch_loss / self.config.training.iterations_per_epoch
                print(f"  Epoch [{epoch+1}/{self.config.training.scr_epochs}] Loss: {avg_loss:.4f}")
        
        # 5. NCM ì—…ë°ì´íŠ¸ (ë©”ëª¨ë¦¬ ë²„í¼ ë°ì´í„°ë§Œ ì‚¬ìš©) ğŸ˜¶â€ğŸŒ«ï¸
        # self._update_ncm() ğŸ˜¶â€ğŸŒ«ï¸
        
        # 6. ë©”ëª¨ë¦¬ ë²„í¼ ì—…ë°ì´íŠ¸ ğŸ˜¶â€ğŸŒ«ï¸
        # ğŸŒˆ ì˜¬ë°”ë¥¸ ìˆœì„œ: ë²„í¼ ë¨¼ì €, NCM ë‚˜ì¤‘!
        print(f"\n=== Before buffer update ===")
        print(f"Buffer size: {len(self.memory_buffer)}")
        print(f"Buffer seen classes: {self.memory_buffer.seen_classes if hasattr(self.memory_buffer, 'seen_classes') else 'N/A'}")
        print(f"image_paths: {type(image_paths)}, len: {len(image_paths)}")
        print(f"original_labels: {type(original_labels)}, len: {len(original_labels)}")
        print(f"original_labels content: {original_labels}")
        
        # ğŸŒˆ Step 1: ë©”ëª¨ë¦¬ ë²„í¼ ì—…ë°ì´íŠ¸ (ë¨¼ì €!)
        self.memory_buffer.update_from_dataset(image_paths, original_labels)  # ì›ë³¸ labels ì‚¬ìš©
        self._full_memory_dataset = None  # ìºì‹œ ë¬´íš¨í™”
        print(f"Memory buffer size after update: {len(self.memory_buffer)}")
        
        # ğŸŒˆ Step 2: NCM ì—…ë°ì´íŠ¸ (ë²„í¼ ì—…ë°ì´íŠ¸ í›„!)
        self._update_ncm()
        
        # ğŸŒˆ ë””ë²„ê¹…: NCMê³¼ ë²„í¼ ë™ê¸°í™” í™•ì¸
        all_paths, all_labels = self.memory_buffer.get_all_data()
        buffer_classes = set(int(label) for label in all_labels)
        ncm_classes = set(self.ncm.class_means_dict.keys())
        missing = buffer_classes - ncm_classes
        
        if missing:
            print(f"âš ï¸  NCM missing classes: {sorted(list(missing))}")
        else:
            print(f"âœ… NCM synchronized: {len(ncm_classes)} classes")
        
        # print(f"Memory buffer size: {len(self.memory_buffer)}") ğŸ˜¶â€ğŸŒ«ï¸
        
        # 7. Experience ì¹´ìš´í„° ì¦ê°€
        self.experience_count += 1
        
        # 8. Scheduler step
        self.scheduler.step()
        
        return {
            'experience': self.experience_count - 1,
            'user_id': user_id,
            'loss': loss_avg.avg,
            'memory_size': len(self.memory_buffer)
        }
    
    @torch.no_grad()
    def _update_ncm(self):
        """
        NCM classifierì˜ class meansë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.
        ë©”ëª¨ë¦¬ ë²„í¼ì˜ ë°ì´í„°ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.
        """
        if len(self.memory_buffer) == 0:
            return
            
        self.model.eval()
        
        # ë©”ëª¨ë¦¬ ë²„í¼ì—ì„œ ëª¨ë“  ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
        all_paths, all_labels = self.memory_buffer.get_all_data()
        
        # ë°ì´í„°ì…‹ ìƒì„±
        dataset = MemoryDataset(
            paths=all_paths,
            labels=all_labels,
            transform=self.test_transform,
            train=False
        )
        
        dataloader = DataLoader(
            dataset,
            batch_size=128,
            shuffle=False,
            num_workers=self.config.training.num_workers
        )
        
        # í´ë˜ìŠ¤ë³„ë¡œ features ìˆ˜ì§‘
        class_features = {}
        
        for data, labels in dataloader:
            # data1ë§Œ ì‚¬ìš© (í…ŒìŠ¤íŠ¸ ì‹œ)
            data = data[0].to(self.device)
            labels = labels.to(self.device)
            
            # Feature extraction
            features = self.model.getFeatureCode(data)
            
            # í´ë˜ìŠ¤ë³„ë¡œ ë¶„ë¥˜
            for i, label in enumerate(labels):
                label_item = label.item()
                if label_item not in class_features:
                    class_features[label_item] = []
                class_features[label_item].append(features[i].cpu())
        
        # í´ë˜ìŠ¤ë³„ í‰ê·  ê³„ì‚°
        class_means = {}
        for label, features_list in class_features.items():
            if len(features_list) > 0:
                mean_feature = torch.stack(features_list).mean(dim=0)
                mean_feature = mean_feature / mean_feature.norm()
                class_means[label] = mean_feature
        
        # NCM ì—…ë°ì´íŠ¸
        # ğŸŒˆ momentum ìˆ˜ì •: ì™„ì „ êµì²´ ë°©ì‹ìœ¼ë¡œ ë³€ê²½
        # self.ncm.update_class_means_dict( ğŸ˜¶â€ğŸŒ«ï¸
        #     class_means,  ğŸ˜¶â€ğŸŒ«ï¸
        #     momentum=self.config.training.ncm_momentum ğŸ˜¶â€ğŸŒ«ï¸
        # ) ğŸ˜¶â€ğŸŒ«ï¸
        
        # ğŸŒˆ ë°©ë²• 1: ì™„ì „ êµì²´ (ì¶”ì²œ)
        self.ncm.replace_class_means_dict(class_means)
        print(f"Updated NCM with {len(class_means)} classes (full replacement)")
        
        # ğŸŒˆ ë°©ë²• 2: ì„ íƒì  momentum (ì˜µì…˜ - í•„ìš”ì‹œ ì£¼ì„ í•´ì œ)
        # for label, new_mean in class_means.items():
        #     if label not in self.ncm.class_means_dict:
        #         # ìƒˆ í´ë˜ìŠ¤: momentum 0
        #         self.ncm.update_class_means_dict({label: new_mean}, momentum=0.0)
        #     else:
        #         # ê¸°ì¡´ í´ë˜ìŠ¤: config momentum ì ìš©
        #         self.ncm.update_class_means_dict(
        #             {label: new_mean}, 
        #             momentum=self.config.training.ncm_momentum
        #         )
        
        # print(f"Updated NCM with {len(class_means)} classes") ğŸ˜¶â€ğŸŒ«ï¸
        
        self.model.train()
    
    def evaluate(self, test_dataset: Dataset) -> float:
        """
        NCMì„ ì‚¬ìš©í•˜ì—¬ ì •í™•ë„ë¥¼ í‰ê°€í•©ë‹ˆë‹¤.
        """
        self.model.eval()
        
        dataloader = DataLoader(
            test_dataset,
            batch_size=128,
            shuffle=False,
            num_workers=self.config.training.num_workers
        )
        
        correct = 0
        total = 0
        
        with torch.no_grad():
            for data, labels in dataloader:
                # data1ë§Œ ì‚¬ìš© (í…ŒìŠ¤íŠ¸ ì‹œ)
                data = data[0].to(self.device)
                labels = labels.to(self.device)
                
                # Feature extraction
                features = self.model.getFeatureCode(data)
                
                # NCM prediction
                predictions = self.ncm.predict(features)
                
                correct += (predictions == labels).sum().item()
                total += labels.size(0)
        
        accuracy = 100.0 * correct / total
        return accuracy
    
    def save_checkpoint(self, path: str):
        """ì²´í¬í¬ì¸íŠ¸ ì €ì¥"""
        torch.save({
            'model_state_dict': self.model.state_dict(),
            'ncm_state_dict': self.ncm.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'experience_count': self.experience_count,
            'memory_buffer_size': len(self.memory_buffer)
        }, path)
    
    def load_checkpoint(self, path: str):
        """ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ"""
        checkpoint = torch.load(path)
        self.model.load_state_dict(checkpoint['model_state_dict'])
        self.ncm.load_state_dict(checkpoint['ncm_state_dict'])
        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
        self.experience_count = checkpoint['experience_count']