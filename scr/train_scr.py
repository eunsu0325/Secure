#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Supervised Contrastive Replay (SCR) Training Script with Open-set Support  # ğŸ‹ ì„¤ëª… ìˆ˜ì •
CCNet + SCR for Continual Learning
"""

import os
import argparse
import time
import numpy as np
from collections import defaultdict
from typing import Dict, List
import json
import random

import torch
from torch.utils.data import DataLoader, Subset

# Project imports
import sys
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from config import ConfigParser
# ğŸ‘» ì‚¬ì „í›ˆë ¨ ë¡œë” import ì¶”ê°€
from utils.pretrained_loader import PretrainedLoader  # ğŸ‘»

from models import ccnet, MyDataset, get_scr_transforms
from scr import (
    ExperienceStream, 
    ClassBalancedBuffer, 
    NCMClassifier, 
    SCRTrainer
)

def fix_random_seed(seed):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)

# ğŸŒ½ BASE_ID ê³„ì‚° í•¨ìˆ˜ ì¶”ê°€
def compute_safe_base_id(*txt_files):
    """ëª¨ë“  txt íŒŒì¼ì—ì„œ ìµœëŒ€ user IDë¥¼ ì°¾ì•„ ì•ˆì „í•œ BASE_ID ê³„ì‚°"""
    max_id = 0
    for path in txt_files:
        with open(path, 'r', encoding='utf-8') as fh:
            for line in fh:
                parts = line.strip().split()
                if len(parts) != 2:
                    continue
                try:
                    lab = int(parts[1])
                except ValueError:
                    continue
                max_id = max(max_id, lab)
    base = max_id + 1
    print(f"[NEG-BASE] max_user_id={max_id}, BASE_ID={base}")
    return base

# ğŸŒ½ purge_negatives í•¨ìˆ˜ ì¶”ê°€
def purge_negatives(memory_buffer, base_id):
    """ë©”ëª¨ë¦¬ ë²„í¼ì—ì„œ ëª¨ë“  ë„¤ê±°í‹°ë¸Œ í´ë˜ìŠ¤ ì œê±°"""
    to_del = [int(c) for c in list(memory_buffer.seen_classes) if int(c) >= base_id]
    removed = 0
    for c in to_del:
        if c in memory_buffer.buffer_groups:
            removed += len(memory_buffer.buffer_groups[c].buffer)
            del memory_buffer.buffer_groups[c]
        memory_buffer.seen_classes.discard(c)
    print(f"ğŸ§¹ Purged {len(to_del)} negative classes ({removed} samples)")

# ğŸŒ½ remove_negative_samples_gradually í•¨ìˆ˜ ìˆ˜ì • (ë²„ê·¸ ìˆ˜ì •)
def remove_negative_samples_gradually(memory_buffer: ClassBalancedBuffer, 
                                    base_id: int,  # ğŸŒ½ base_id íŒŒë¼ë¯¸í„° ì¶”ê°€
                                    removal_ratio: float = 0.2):
    """
    ë©”ëª¨ë¦¬ ë²„í¼ì—ì„œ negative ìƒ˜í”Œì„ ì ì§„ì ìœ¼ë¡œ ì œê±°
    
    :param removal_ratio: ì œê±°í•  ë¹„ìœ¨ (0.2 = 20%)
    """
    # negative_classes = [c for c in memory_buffer.seen_classes if c < 0]  # ğŸªµ ë²„ê·¸: ë„¤ê±°í‹°ë¸ŒëŠ” ìŒìˆ˜ê°€ ì•„ë‹˜
    negative_classes = [int(c) for c in memory_buffer.seen_classes if int(c) >= base_id]  # ğŸŒ½ ìˆ˜ì •
    
    if not negative_classes:
        return 0
    
    # ì œê±°í•  í´ë˜ìŠ¤ ìˆ˜ ê³„ì‚°
    num_to_remove = max(1, int(len(negative_classes) * removal_ratio))
    
    # ëœë¤í•˜ê²Œ ì„ íƒí•˜ì—¬ ì œê±°
    classes_to_remove = np.random.choice(negative_classes, size=num_to_remove, replace=False)
    
    removed_count = 0
    for class_id in classes_to_remove:
        if class_id in memory_buffer.buffer_groups:
            # í•´ë‹¹ í´ë˜ìŠ¤ì˜ ìƒ˜í”Œ ìˆ˜
            removed_count += len(memory_buffer.buffer_groups[class_id].buffer)
            # ë²„í¼ì—ì„œ ì œê±°
            del memory_buffer.buffer_groups[class_id]
            memory_buffer.seen_classes.remove(class_id)
    
    print(f"Removed {num_to_remove} negative classes ({removed_count} samples)")
    return removed_count

class ContinualLearningEvaluator:
    """
    Continual Learning í‰ê°€ ë©”íŠ¸ë¦­ ê³„ì‚°
    - Average Accuracy
    - Forgetting Measure
    ğŸ‹ - Open-set metrics (TAR, TRR, FAR)
    """
    def __init__(self, num_experiences: int):
        self.num_experiences = num_experiences
        self.accuracy_history = defaultdict(list)  # {exp_id: [acc1, acc2, ...]}
        self.openset_history = []  # ğŸ‹ ì˜¤í”ˆì…‹ ë©”íŠ¸ë¦­ íˆìŠ¤í† ë¦¬
        
    def update(self, experience_id: int, accuracy: float):
        """experience_id í•™ìŠµ í›„ ì •í™•ë„ ì—…ë°ì´íŠ¸"""
        self.accuracy_history[experience_id].append(accuracy)
    
    def update_openset(self, metrics: dict):  # ğŸ‹ ìƒˆ ë©”ì„œë“œ
        """ì˜¤í”ˆì…‹ ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸"""
        self.openset_history.append(metrics)
    
    def get_average_accuracy(self) -> float:
        """í˜„ì¬ê¹Œì§€ì˜ í‰ê·  ì •í™•ë„"""
        all_accs = []
        for accs in self.accuracy_history.values():
            if accs:
                all_accs.append(accs[-1])  # ê° experienceì˜ ìµœì‹  ì •í™•ë„
        return np.mean(all_accs) if all_accs else 0.0
    
    def get_forgetting_measure(self) -> float:
        """
        Forgetting Measure ê³„ì‚°
        ê° experienceì— ëŒ€í•´: max(ì´ì „ ì •í™•ë„) - í˜„ì¬ ì •í™•ë„
        """
        forgetting = []
        for exp_id, accs in self.accuracy_history.items():
            if len(accs) > 1:
                max_acc = max(accs[:-1])
                curr_acc = accs[-1]
                forgetting.append(max_acc - curr_acc)
        
        return np.mean(forgetting) if forgetting else 0.0
    
    def get_latest_openset_metrics(self) -> dict:  # ğŸ‹ ìƒˆ ë©”ì„œë“œ
        """ìµœì‹  ì˜¤í”ˆì…‹ ë©”íŠ¸ë¦­ ë°˜í™˜"""
        if self.openset_history:
            return self.openset_history[-1]
        return {}


def evaluate_on_test_set(trainer: SCRTrainer, config, openset_mode=False) -> tuple:  # ğŸ‹ ìˆ˜ì •
    """
    test_set_fileì„ ì‚¬ìš©í•œ ì „ì²´ í‰ê°€
    ğŸ‹ openset_mode=Trueë©´ ì˜¤í”ˆì…‹ í‰ê°€ë„ ìˆ˜í–‰
    
    Returns:
        (accuracy, openset_metrics) if openset_mode else (accuracy, None)
    """
    # ì „ì²´ í…ŒìŠ¤íŠ¸ì…‹ ë¡œë“œ
    test_dataset = MyDataset(
        txt=config.dataset.test_set_file,
        transforms=get_scr_transforms(
            train=False,
            imside=config.dataset.height,
            channels=config.dataset.channels
        ),
        train=False,
        imside=config.dataset.height,
        outchannels=config.dataset.channels
    )
    
    # í˜„ì¬ê¹Œì§€ í•™ìŠµí•œ í´ë˜ìŠ¤ë§Œ í•„í„°ë§
    known_classes = set(trainer.ncm.class_means_dict.keys())
    
    if not known_classes:
        return (0.0, {}) if openset_mode else (0.0, None)  # ğŸ‹
    
    # ğŸ‹ ì˜¤í”ˆì…‹ ëª¨ë“œ: Knownê³¼ Unknown ë¶„ë¦¬
    if openset_mode and trainer.openset_enabled:
        known_indices = []
        unknown_indices = []
        
        for i in range(len(test_dataset)):
            label = int(test_dataset.images_label[i])
            if label in known_classes:
                known_indices.append(i)
            else:
                unknown_indices.append(i)
        
        # Known í‰ê°€ (Closed-set accuracy)
        if known_indices:
            known_subset = Subset(test_dataset, known_indices)
            accuracy = trainer.evaluate(known_subset)
        else:
            accuracy = 0.0
        
        # ğŸ‹ ì˜¤í”ˆì…‹ ë©”íŠ¸ë¦­ ê³„ì‚°
        openset_metrics = {}
        
        if known_indices and trainer.ncm.tau_s is not None:
            # Knownì—ì„œ TAR/FRR ê³„ì‚°
            from utils.utils_openset import predict_batch
            
            # ìƒ˜í”Œë§ (ë„ˆë¬´ ë§ìœ¼ë©´)
            if len(known_indices) > 500:
                known_indices = np.random.choice(known_indices, 500, replace=False)
            
            known_paths = [test_dataset.images_path[i] for i in known_indices] 
            known_labels = [int(test_dataset.images_label[i]) for i in known_indices]
            
            preds = predict_batch(
                trainer.model, trainer.ncm,
                known_paths, trainer.test_transform, trainer.device
            )
            
            correct = sum(1 for p, l in zip(preds, known_labels) if p == l)
            rejected = sum(1 for p in preds if p == -1)
            
            openset_metrics['TAR'] = correct / max(1, len(preds))
            openset_metrics['FRR'] = rejected / max(1, len(preds))
        
        if unknown_indices and trainer.ncm.tau_s is not None:
            # Unknownì—ì„œ TRR/FAR ê³„ì‚°
            from utils.utils_openset import predict_batch
            
            # ìƒ˜í”Œë§
            if len(unknown_indices) > 500:
                unknown_indices = np.random.choice(unknown_indices, 500, replace=False)
            
            unknown_paths = [test_dataset.images_path[i] for i in unknown_indices]  # ğŸ‹
            
            preds = predict_batch(
                trainer.model, trainer.ncm,
                unknown_paths, trainer.test_transform, trainer.device
            )
            
            openset_metrics['TRR_unknown'] = sum(1 for p in preds if p == -1) / len(preds)
            openset_metrics['FAR_unknown'] = 1 - openset_metrics['TRR_unknown']
        
        openset_metrics['tau_s'] = trainer.ncm.tau_s
        openset_metrics['tau_m'] = trainer.ncm.tau_m if trainer.ncm.use_margin else None
        
        print(f"Evaluating on {len(known_indices)} known + {len(unknown_indices)} unknown samples")
        
        return accuracy, openset_metrics
    
    else:
        # ğŸ‹ ê¸°ì¡´ ë°©ì‹ (Closed-setë§Œ)
        # í•„í„°ë§ëœ ì¸ë±ìŠ¤ ì°¾ê¸°
        filtered_indices = []
        for i in range(len(test_dataset)):
            label = int(test_dataset.images_label[i])
            if label in known_classes:
                filtered_indices.append(i)
        
        if not filtered_indices:
            return (0.0, None)  # ğŸ‹
        
        # Subset ìƒì„±
        filtered_test = Subset(test_dataset, filtered_indices)
        
        print(f"Evaluating on {len(filtered_indices)} test samples from {len(known_classes)} classes")
        
        # í‰ê°€
        accuracy = trainer.evaluate(filtered_test)  # ğŸ‹
        return accuracy, None  # ğŸ‹


def main(args):
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    
    # 1. Configuration ë¡œë“œ
    config = ConfigParser(args.config)
    # ğŸ‘» config ê°ì²´ ê°€ì ¸ì˜¤ê¸°
    config_obj = config.get_config()  # ğŸ‘»
    print(f"Using config: {args.config}")
    print(config)
    
    # ğŸŒ½ BASE_ID ê³„ì‚° ë° ì„¤ì •
    config_obj.negative.base_id = compute_safe_base_id(
        config_obj.dataset.train_set_file,
        config_obj.dataset.test_set_file
    )
    
    # ğŸ‹ ì˜¤í”ˆì…‹ ëª¨ë“œ í™•ì¸
    openset_enabled = hasattr(config_obj, 'openset') and config_obj.openset.enabled
    if openset_enabled:
        print("\nğŸ‹ ========== OPEN-SET MODE ENABLED ==========")
        print(f"   Warmup users: {config_obj.openset.warmup_users}")
        print(f"   Initial tau: {config_obj.openset.initial_tau}")
        print(f"   Margin: {config_obj.openset.use_margin} (tau={config_obj.openset.margin_tau})")
        print("ğŸ‹ =========================================\n")
    
    # GPU ì„¤ì •
    device = torch.device(
        f"cuda:{config_obj.training.gpu_ids}" 
        if torch.cuda.is_available() and not args.no_cuda 
        else "cpu"
    )
    print(f'Device: {device}')
    
    # Random seed ê³ ì •
    if args.seed is not None:
        fix_random_seed(args.seed)
    
    # 2. ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„±
    results_dir = os.path.join(config_obj.training.results_path, 'scr_results')
    if openset_enabled:  # ğŸ‹
        results_dir = os.path.join(config_obj.training.results_path, 'scr_openset_results')
    os.makedirs(results_dir, exist_ok=True)
    
    # 3. ë°ì´í„° ìŠ¤íŠ¸ë¦¼ ì´ˆê¸°í™”
    print("\n=== Initializing Data Stream ===")
    data_stream = ExperienceStream(
        train_file=config_obj.dataset.train_set_file,
        negative_file=config_obj.dataset.negative_samples_file,
        num_negative_classes=config_obj.dataset.num_negative_classes,
        base_id=config_obj.negative.base_id  # ğŸŒ½ base_id ì „ë‹¬
    )
    
    stats = data_stream.get_statistics()
    print(f"Total users: {stats['num_users']}")
    print(f"Samples per user: {stats['samples_per_user']}")
    print(f"Negative samples: {stats['negative_samples']}")
    
    # 4. ëª¨ë¸ ë° ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
    print("\n=== Initializing Model and Components ===")
    
    # CCNet ëª¨ë¸
    model = ccnet(weight=config_obj.model.competition_weight)  # ğŸ‘» device ì´ë™ ì „ì— ìƒì„±
    
    # ğŸ‘» ì‚¬ì „í›ˆë ¨ ê°€ì¤‘ì¹˜ ë¡œë“œ (device ì´ë™ ì „ì—!)
    if hasattr(config_obj.model, 'use_pretrained') and config_obj.model.use_pretrained:  # ğŸ‘»
        if config_obj.model.pretrained_path and config_obj.model.pretrained_path.exists():  # ğŸ‘»
            print(f"\nğŸ“¦ Loading pretrained weights from main script...")  # ğŸ‘»
            loader = PretrainedLoader()  # ğŸ‘»
            try:  # ğŸ‘»
                model = loader.load_ccnet_pretrained(  # ğŸ‘»
                    model=model,  # ğŸ‘»
                    checkpoint_path=config_obj.model.pretrained_path,  # ğŸ‘»
                    device=device,  # ğŸ‘»
                    verbose=True  # ğŸ‘»
                )  # ğŸ‘»
                print("âœ… Pretrained weights loaded successfully!")  # ğŸ‘»
            except Exception as e:  # ğŸ‘»
                print(f"âš ï¸  Failed to load pretrained model: {e}")  # ğŸ‘»
                print("Continuing with random initialization...")  # ğŸ‘»
        else:  # ğŸ‘»
            print(f"âš ï¸  Pretrained path not found or not set")  # ğŸ‘»
    else:  # ğŸ‘»
        print("ğŸ² Starting from random initialization")  # ğŸ‘»
    
    # ğŸ‘» ëª¨ë¸ì„ deviceë¡œ ì´ë™
    model = model.to(device)  # ğŸ‘»
    
    # NCM Classifier
    ncm_classifier = NCMClassifier(normalize=True).to(device)  # ğŸ‹ ì½”ì‚¬ì¸ ëª¨ë“œë¡œ ë³€ê²½
    
    # Memory Buffer
    memory_buffer = ClassBalancedBuffer(
        max_size=config_obj.training.memory_size,
        min_samples_per_class=config_obj.training.min_samples_per_class
    )
    
    # SCR Trainer
    # ğŸ‘» config_obj ì „ë‹¬
    trainer = SCRTrainer(
        model=model,
        ncm_classifier=ncm_classifier,
        memory_buffer=memory_buffer,
        config=config_obj,  # ğŸ‘» config â†’ config_obj
        device=device
    )
    
    # 5. Negative ìƒ˜í”Œë¡œ ì´ˆê¸°í™” ğŸ£
    print("\n=== Initializing with Negative Samples ===")
    neg_paths, neg_labels = data_stream.get_negative_samples()
    
    # memory_batch_sizeë§Œí¼ë§Œ ì„ íƒ
    if len(neg_paths) > config_obj.training.memory_batch_size:
        selected_indices = np.random.choice(
            len(neg_paths), 
            size=config_obj.training.memory_batch_size,
            replace=False
        )
        neg_paths = [neg_paths[i] for i in selected_indices]
        neg_labels = [neg_labels[i] for i in selected_indices]
    
    # ë©”ëª¨ë¦¬ ë²„í¼ ì´ˆê¸°í™”
    memory_buffer.update_from_dataset(neg_paths, neg_labels)
    print(f"Initial buffer size: {len(memory_buffer)}")
    
    # NCM ì´ˆê¸°í™” ğŸ£
    print("ğŸ„ NCM starts empty - no fake class contamination")  # ğŸ„
    
    # 6. í‰ê°€ì ì´ˆê¸°í™”
    evaluator = ContinualLearningEvaluator(num_experiences=config_obj.training.num_experiences)
    
    # 7. í•™ìŠµ ê²°ê³¼ ì €ì¥ìš©
    training_history = {
        'losses': [],
        'accuracies': [],
        'forgetting_measures': [],
        'memory_sizes': [],
        'negative_removal_history': [],
        'openset_metrics': [],  # ğŸ‹ ì¶”ê°€
        # ğŸ‘» ì‚¬ì „í›ˆë ¨ ì •ë³´ ì¶”ê°€
        'pretrained_used': config_obj.model.use_pretrained,  # ğŸ‘»
        'pretrained_path': str(config_obj.model.pretrained_path) if config_obj.model.pretrained_path else None,  # ğŸ‘»
        'openset_enabled': openset_enabled  # ğŸ‹ ì¶”ê°€
    }
    
    # 8. Continual Learning ì‹œì‘
    print("\n=== Starting Continual Learning ===")
    print(f"Total experiences: {config_obj.training.num_experiences}")
    print(f"Evaluation interval: every {config_obj.training.test_interval} users")
    print(f"ğŸ”¥ Negative warmup: exp0~{config_obj.negative.warmup_experiences-1}")  # ğŸŒ½
    # ğŸ‘» ì¤‘ìš” íŒŒë¼ë¯¸í„° ì¶œë ¥
    print(f"Learning rate: {config_obj.training.learning_rate}")  # ğŸ‘»
    print(f"Memory batch size: {config_obj.training.memory_batch_size}")  # ğŸ‘»
    print(f"Temperature: {config_obj.training.temperature}")  # ğŸ‘»
    
    start_time = time.time()
    
    for exp_id, (user_id, image_paths, labels) in enumerate(data_stream):
        
        # Experience í•™ìŠµ
        stats = trainer.train_experience(user_id, image_paths, labels)
        training_history['losses'].append(stats['loss'])
        training_history['memory_sizes'].append(stats['memory_size'])
        
        # ğŸŒ½ exp3â†’exp4 ê²½ê³„ì—ì„œ ë„¤ê±°í‹°ë¸Œ ì™„ì „ ì œê±°
        if exp_id + 1 == config_obj.negative.warmup_experiences:
            print(f"\nğŸ”¥ === Warmup End (exp{exp_id}) â†’ Post-warmup (exp{exp_id+1}) ===")
            
            # í‰ê°€ (purge ì „)
            acc_pre, _ = evaluate_on_test_set(trainer, config_obj, openset_mode=openset_enabled)
            print(f"[Warmup-End] pre-purge ACC={acc_pre:.2f}%")
            
            # ë„¤ê±°í‹°ë¸Œ ì œê±°
            purge_negatives(memory_buffer, config_obj.negative.base_id)
            trainer._update_ncm()
            
            # í‰ê°€ (purge í›„)
            acc_post, _ = evaluate_on_test_set(trainer, config_obj, openset_mode=openset_enabled)
            print(f"[Warmup-End] post-purge ACC={acc_post:.2f}%")
            print(f"ğŸ”¥ ========================================\n")
        
        # í‰ê°€ ì£¼ê¸° í™•ì¸
        if (exp_id + 1) % config_obj.training.test_interval == 0 or exp_id == config_obj.training.num_experiences - 1:
            
            print(f"\n=== Evaluation at Experience {exp_id + 1} ===")
            
            # í…ŒìŠ¤íŠ¸ì…‹ìœ¼ë¡œ í‰ê°€
            accuracy, openset_metrics = evaluate_on_test_set(  # ğŸ‹
                trainer, config_obj, 
                openset_mode=openset_enabled
            )
            
            # ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
            evaluator.update(exp_id, accuracy)
            if openset_metrics:  # ğŸ‹
                evaluator.update_openset(openset_metrics)
            
            # í‰ê·  ì •í™•ë„ì™€ Forgetting ê³„ì‚°
            avg_acc = evaluator.get_average_accuracy()
            forgetting = evaluator.get_forgetting_measure()
            
            # ğŸ‹ ê¸°ë¡ ì €ì¥
            accuracy_record = {
                'experience': exp_id + 1,
                'accuracy': accuracy,
                'average_accuracy': avg_acc,
                'forgetting': forgetting
            }
            
            # ğŸ‹ ì˜¤í”ˆì…‹ ë©”íŠ¸ë¦­ ì¶”ê°€
            if openset_metrics:
                accuracy_record.update(openset_metrics)
                training_history['openset_metrics'].append(openset_metrics)
            
            training_history['accuracies'].append(accuracy_record)
            
            print(f"Test Accuracy: {accuracy:.2f}%")
            print(f"Average Accuracy: {avg_acc:.2f}%")
            print(f"Forgetting Measure: {forgetting:.2f}%")
            print(f"Memory Buffer Size: {len(memory_buffer)}")
            
            # ğŸ‹ ì˜¤í”ˆì…‹ ë©”íŠ¸ë¦­ ì¶œë ¥
            if openset_metrics:
                print(f"\nğŸ‹ Open-set Metrics:")
                if 'TAR' in openset_metrics:
                    print(f"   TAR: {openset_metrics['TAR']:.3f}, FRR: {openset_metrics['FRR']:.3f}")
                if 'TRR_unknown' in openset_metrics:
                    print(f"   TRR_unknown: {openset_metrics['TRR_unknown']:.3f}, FAR_unknown: {openset_metrics['FAR_unknown']:.3f}")
                print(f"   Ï„_s: {openset_metrics.get('tau_s', 0):.4f}")
            
            # ğŸ‹ Trainerì˜ ì˜¤í”ˆì…‹ í‰ê°€ íˆìŠ¤í† ë¦¬ë„ ì €ì¥
            if hasattr(trainer, 'evaluation_history') and trainer.evaluation_history:
                training_history['trainer_openset_history'] = trainer.evaluation_history
            
            # ì²´í¬í¬ì¸íŠ¸ ì €ì¥
            checkpoint_path = os.path.join(
                results_dir, 
                f'checkpoint_exp_{exp_id + 1}.pth'
            )
            trainer.save_checkpoint(checkpoint_path)
            
        # ì§„í–‰ ìƒí™© ì¶œë ¥
        if (exp_id + 1) % 10 == 0:
            elapsed = time.time() - start_time
            eta = elapsed / (exp_id + 1) * (config_obj.training.num_experiences - exp_id - 1)
            print(f"Progress: {exp_id + 1}/{config_obj.training.num_experiences} "
                  f"({100 * (exp_id + 1) / config_obj.training.num_experiences:.1f}%) "
                  f"| Elapsed: {elapsed/60:.1f}m | ETA: {eta/60:.1f}m")
    
    # 9. ìµœì¢… ê²°ê³¼ ì €ì¥
    print("\n=== Saving Final Results ===")
    
    # í•™ìŠµ ê¸°ë¡ ì €ì¥
    history_path = os.path.join(results_dir, 'training_history.json')
    with open(history_path, 'w') as f:
        json.dump(training_history, f, indent=4)
    
    # ìµœì¢… í†µê³„
    final_result = training_history['accuracies'][-1] if training_history['accuracies'] else {}
    final_acc = final_result.get('accuracy', 0)
    final_avg_acc = final_result.get('average_accuracy', 0)
    final_forget = final_result.get('forgetting', 0)
    
    print(f"\n=== Final Results ===")
    print(f"Final Test Accuracy: {final_acc:.2f}%")
    print(f"Final Average Accuracy: {final_avg_acc:.2f}%")
    print(f"Final Forgetting Measure: {final_forget:.2f}%")
    
    # ğŸ‹ ìµœì¢… ì˜¤í”ˆì…‹ ë©”íŠ¸ë¦­
    if openset_enabled and 'TAR' in final_result:
        print(f"\nğŸ‹ Final Open-set Performance:")
        print(f"   TAR: {final_result.get('TAR', 0):.3f}")
        print(f"   TRR (Unknown): {final_result.get('TRR_unknown', 0):.3f}")
        print(f"   FAR (Unknown): {final_result.get('FAR_unknown', 0):.3f}")
        print(f"   Final Ï„_s: {final_result.get('tau_s', 0):.4f}")
    
    print(f"\nTotal Training Time: {(time.time() - start_time)/60:.1f} minutes")
    
    # ê²°ê³¼ ìš”ì•½ ì €ì¥
    summary = {
        'config': args.config,
        'num_experiences': config_obj.training.num_experiences,
        'memory_size': config_obj.training.memory_size,
        'final_accuracy': final_acc,
        'final_average_accuracy': final_avg_acc,
        'final_forgetting': final_forget,
        'total_time_minutes': (time.time() - start_time) / 60,
        'negative_removal_history': training_history['negative_removal_history'],
        'openset_enabled': openset_enabled  # ğŸ‹
    }
    
    # ğŸ‹ ì˜¤í”ˆì…‹ ìš”ì•½ ì¶”ê°€
    if openset_enabled and final_result:
        summary['final_openset'] = {
            'TAR': final_result.get('TAR', 0),
            'TRR_unknown': final_result.get('TRR_unknown', 0),
            'FAR_unknown': final_result.get('FAR_unknown', 0),
            'tau_s': final_result.get('tau_s', 0)
        }
    
    summary_path = os.path.join(results_dir, 'summary.json')
    with open(summary_path, 'w') as f:
        json.dump(summary, f, indent=4)
    
    print(f"\nResults saved to: {results_dir}")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='SCR Training for CCNet with Open-set Support')  # ğŸ‹ ì„¤ëª… ìˆ˜ì •
    parser.add_argument('--config', type=str, default='config/config.yaml',
                        help='Path to config file')
    parser.add_argument('--seed', type=int, default=42,
                        help='Random seed')
    parser.add_argument('--no-cuda', action='store_true',
                        help='Disable CUDA')
    
    args = parser.parse_args()
    main(args)