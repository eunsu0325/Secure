# ğŸŒ• Avalancheì˜ ncm_classifier.py ê¸°ë°˜ (ìµœì í™” + ë²„ê·¸ ìˆ˜ì •)

from typing import Dict
import torch
from torch import Tensor, nn
import torch.nn.functional as F


class NCMClassifier(nn.Module):
    """
    NCM (Nearest Class Mean) Classifier - ìµœì í™” ë²„ì „.
    
    ğŸš€ ìµœì í™”: cdist ëŒ€ì‹  í–‰ë ¬ê³± ì‚¬ìš© (2-3ë°° ë¹ ë¦„)
    âœ… normalize=True: ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê¸°ë°˜
    âœ… normalize=False: ìœ í´ë¦¬ë””ì•ˆ ê±°ë¦¬ ê¸°ë°˜
    """

    def __init__(self, normalize: bool = True):
        super().__init__()
        self.register_buffer("class_means", None)
        self.class_means_dict = {}
        self.normalize = normalize
        self.max_class = -1
        
        # ğŸ‹ === Open-set ê´€ë ¨ ì¶”ê°€ ===
        self.tau_s = None            # ì „ì—­ ì„ê³„ì¹˜ (ì½”ì‚¬ì¸ ê¸°ì¤€ ê¶Œì¥)
        self.use_margin = False     # ë§ˆì§„ ê·œì¹™ ì‚¬ìš© ì—¬ë¶€
        self.tau_m = 0.05           # ë§ˆì§„ ì„ê³„ì¹˜
        self.unknown_id = -1        # Unknown í´ë˜ìŠ¤ ID
        
        # ğŸ‹ (ì„ íƒ) Z-norm
        self.use_znorm = False
        self.impostor_stats = {}    # {class_id: {'mean':..., 'std':...}}

    def load_state_dict(self, state_dict, strict: bool = True):
        """ì²´í¬í¬ì¸íŠ¸ì—ì„œ ìƒíƒœë¥¼ ë¡œë“œí•©ë‹ˆë‹¤."""
        self.class_means = state_dict["class_means"]
        super().load_state_dict(state_dict, strict)
        if self.class_means is not None:
            for i in range(self.class_means.shape[0]):
                if (self.class_means[i] != 0).any():
                    self.class_means_dict[i] = self.class_means[i].clone()
        self.max_class = max(self.class_means_dict.keys()) if self.class_means_dict else -1

    def _vectorize_means_dict(self):
        """ë”•ì…”ë„ˆë¦¬ë¥¼ í…ì„œë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
        if self.class_means_dict == {}:
            return

        max_class = max(self.class_means_dict.keys())
        self.max_class = max(max_class, self.max_class)
        
        first_mean = list(self.class_means_dict.values())[0]
        feature_size = first_mean.size(0)
        device = first_mean.device
        
        self.class_means = torch.zeros(self.max_class + 1, feature_size).to(device)
        
        for k, v in self.class_means_dict.items():
            self.class_means[k] = self.class_means_dict[k].clone()

    @torch.no_grad()
    def forward(self, x):
        """
        ğŸš€ ìµœì í™”ëœ NCM ë¶„ë¥˜
        
        normalize=True: ì½”ì‚¬ì¸ ìœ ì‚¬ë„ (ì •ê·œí™” í›„ ë‚´ì )
        normalize=False: ìœ í´ë¦¬ë””ì•ˆ ê±°ë¦¬ (ì œê³± ê±°ë¦¬ ì‚¬ìš©)
        """
        # ğŸ„ NCMì´ ë¹„ì–´ìˆìœ¼ë©´ ë¹ˆ ì ìˆ˜ ë°˜í™˜
        if self.class_means_dict == {}:  # ğŸ„
            return torch.zeros((x.shape[0], 0), device=x.device, dtype=x.dtype)  # ğŸ„

        assert self.class_means_dict != {}, "no class means available."
        
        # dtype ì¼ì¹˜ ë³´ì¥ (fp16/AMP ì§€ì›)
        M = self.class_means.to(device=x.device, dtype=x.dtype)
        
        if self.normalize:
            # ğŸŒˆ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê¸°ë°˜
            x = F.normalize(x, p=2, dim=1, eps=1e-12)
            # Më„ ì´ë¯¸ ì •ê·œí™”ë˜ì–´ ìˆìŒ (replace/updateì—ì„œ ì²˜ë¦¬)
            scores = x @ M.T  # (B, C)
            return scores  # ë†’ì„ìˆ˜ë¡ ê°€ê¹Œì›€
            
        else:
            # ğŸŒˆ ìœ í´ë¦¬ë””ì•ˆ ê±°ë¦¬ ê¸°ë°˜ (ë¹ ë¥¸ ë²„ì „)
            # -||x - m||Â² = -||x||Â² - ||m||Â² + 2xÂ·m
            x2 = (x * x).sum(dim=1, keepdim=True)      # (B, 1)
            m2 = (M * M).sum(dim=1, keepdim=False)      # (C,)
            xm = x @ M.T                                # (B, C)
            scores = -(x2 + m2.unsqueeze(0) - 2 * xm)  # (B, C)
            return scores  # ë†’ì„ìˆ˜ë¡ ê°€ê¹Œì›€ (negative distance)

    @torch.no_grad()
    def forward_cdist(self, x):
        """ê¸°ì¡´ cdist ë°©ì‹ (ë¹„êµìš©)"""
        if self.class_means_dict == {}:
            self.init_missing_classes(range(self.max_class + 1), x.shape[1], x.device)

        assert self.class_means_dict != {}, "no class means available."
        
        M = self.class_means.to(device=x.device, dtype=x.dtype)
        
        if self.normalize:
            x = F.normalize(x, p=2, dim=1, eps=1e-12)
            # Më„ ì •ê·œí™”ë˜ì–´ ìˆìŒ
        
        # cdistë¡œ ê±°ë¦¬ ê³„ì‚°
        sqd = torch.cdist(x, M)  # (B, C)
        return -sqd  # negative distance

    def update_class_means_dict(self, class_means_dict: Dict[int, Tensor], momentum: float = 0.5):
        """
        í´ë˜ìŠ¤ í‰ê· ì„ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.
        ğŸŒˆ normalize=Trueë©´ í”„ë¡œí† íƒ€ì…ë„ ìë™ ì •ê·œí™”
        """
        assert 0 <= momentum <= 1
        assert isinstance(class_means_dict, dict)
        
        for k, v in class_means_dict.items():
            if k not in self.class_means_dict or (self.class_means_dict[k] == 0).all():
                # ìƒˆë¡œìš´ í´ë˜ìŠ¤
                self.class_means_dict[k] = class_means_dict[k].clone()
            else:
                # ê¸°ì¡´ í´ë˜ìŠ¤ ì—…ë°ì´íŠ¸
                device = self.class_means_dict[k].device
                self.class_means_dict[k] = (
                    momentum * class_means_dict[k].to(device)
                    + (1 - momentum) * self.class_means_dict[k]
                )
        
        # ğŸŒˆ ë°©ì–´ì  ì •ê·œí™” (normalize=Trueì¼ ë•Œ)
        if self.normalize:
            for k in self.class_means_dict:
                self.class_means_dict[k] = F.normalize(
                    self.class_means_dict[k], p=2, dim=0, eps=1e-12
                )
        
        self._vectorize_means_dict()

    def replace_class_means_dict(self, class_means_dict: Dict[int, Tensor]):
        """
        ê¸°ì¡´ í‰ê· ì„ ì™„ì „íˆ êµì²´í•©ë‹ˆë‹¤.
        ğŸŒˆ normalize=Trueë©´ í”„ë¡œí† íƒ€ì…ë„ ìë™ ì •ê·œí™”
        """
        assert isinstance(class_means_dict, dict)
        
        self.class_means_dict = {k: v.clone() for k, v in class_means_dict.items()}
        
        # ğŸŒˆ ë°©ì–´ì  ì •ê·œí™” (normalize=Trueì¼ ë•Œ)
        if self.normalize:
            for k in self.class_means_dict:
                self.class_means_dict[k] = F.normalize(
                    self.class_means_dict[k], p=2, dim=0, eps=1e-12
                )
        
        self._vectorize_means_dict()

    def init_missing_classes(self, classes, class_size, device):
        """ì•„ì§ í‰ê· ì´ ì—†ëŠ” í´ë˜ìŠ¤ë¥¼ 0 ë²¡í„°ë¡œ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""
        for k in classes:
            if k not in self.class_means_dict:
                self.class_means_dict[k] = torch.zeros(class_size).to(device)
        self._vectorize_means_dict()

    def adaptation(self, experience):
        """ìƒˆë¡œìš´ experienceì— ë§ì¶° ì ì‘í•©ë‹ˆë‹¤."""
        if hasattr(experience, 'classes_in_this_experience'):
            classes = experience.classes_in_this_experience
            for k in classes:
                self.max_class = max(k, self.max_class)
            
            if self.class_means is not None:
                self.init_missing_classes(
                    classes, self.class_means.shape[1], self.class_means.device
                )
    
    def predict(self, x):
        """í´ë˜ìŠ¤ ì˜ˆì¸¡ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
        # ğŸ„ NCMì´ ë¹„ì–´ìˆìœ¼ë©´ -1 ë°˜í™˜
        if len(self.class_means_dict) == 0:  # ğŸ„
            return torch.full((x.shape[0],), -1, dtype=torch.long, device=x.device)  # ğŸ„
        
        scores = self.forward(x)
        return scores.argmax(dim=1)
    
    def get_num_classes(self):
        """í˜„ì¬ ì €ì¥ëœ í´ë˜ìŠ¤ ìˆ˜ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
        return len(self.class_means_dict)
    
    def get_class_means(self):
        """í˜„ì¬ ì €ì¥ëœ í´ë˜ìŠ¤ í‰ê· ë“¤ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
        return self.class_means_dict.copy()
    
    # ğŸ‹ === Open-set ê´€ë ¨ ë©”ì„œë“œ ì¶”ê°€ ===
    
    def set_thresholds(self, tau_s: float, use_margin: bool = False, tau_m: float = 0.05):
        """
        ğŸ‹ ì˜¤í”ˆì…‹ ì„ê³„ì¹˜ ì„¤ì •
        
        Args:
            tau_s: ì „ì—­ ì„ê³„ì¹˜ (ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê¸°ì¤€)
            use_margin: ë§ˆì§„ ê·œì¹™ ì‚¬ìš© ì—¬ë¶€
            tau_m: ë§ˆì§„ ì„ê³„ì¹˜ (1st-2nd ì°¨ì´)
        """
        self.tau_s = float(tau_s)
        self.use_margin = bool(use_margin)
        self.tau_m = float(tau_m)
        
    def enable_znorm(self, enabled: bool):
        """ğŸ‹ Z-norm í™œì„±í™”/ë¹„í™œì„±í™”"""
        self.use_znorm = bool(enabled)
        
    def update_impostor_stats(self, class_id: int, mean: float, std: float):
        """ğŸ‹ Z-normìš© í´ë˜ìŠ¤ë³„ impostor í†µê³„ ì—…ë°ì´íŠ¸"""
        self.impostor_stats[class_id] = {
            'mean': float(mean),
            'std': float(std)
        }
    
    @torch.no_grad()
    def predict_openset(self, x):
        """
        ğŸ‹ ì˜¤í”ˆì…‹ ì˜ˆì¸¡ (ì „ì—­ ì„ê³„ì¹˜ + ì„ íƒì  ë§ˆì§„)
        
        Args:
            x: (B, D) ì„ë² ë”© (ì½”ì‚¬ì¸ ë²„ì „ì´ë©´ L2 ì •ê·œí™” ê°€ì •)
            
        Returns:
            (B,) ì˜ˆì¸¡ í´ë˜ìŠ¤ (ê±°ë¶€ëŠ” -1)
        """
        # ğŸ„ NCMì´ ë¹„ì–´ìˆìœ¼ë©´ ëª¨ë‘ -1 ë°˜í™˜
        if len(self.class_means_dict) == 0:  # ğŸ„
            return torch.full((x.shape[0],), -1, dtype=torch.long, device=x.device)  # ğŸ„
        
        # ì ìˆ˜ ê³„ì‚°
        scores = self.forward(x)  # (B, C)
        
        # Top-2 ì¶”ì¶œ
        top2 = scores.topk(2, dim=1)  # values: (B,2), indices: (B,2)
        max_score = top2.values[:, 0]
        second_score = top2.values[:, 1] if scores.shape[1] > 1 else torch.zeros_like(max_score)
        pred = top2.indices[:, 0]
        
        # ğŸ‹ (ì„ íƒ) Z-norm: í´ë˜ìŠ¤ë³„ impostor í†µê³„ë¡œ ì •ê·œí™”
        if self.use_znorm and len(self.impostor_stats) > 0:
            z_scores = []
            for i in range(len(pred)):
                k = int(pred[i].item())
                if k in self.impostor_stats:
                    mu = self.impostor_stats[k]['mean']
                    sd = self.impostor_stats[k]['std'] + 1e-6
                    z_scores.append((max_score[i].item() - mu) / sd)
                else:
                    z_scores.append(max_score[i].item())  # fallback
            max_score = torch.tensor(z_scores, device=max_score.device, dtype=max_score.dtype)
        
        # ğŸ‹ ì „ì—­ ì„ê³„ì¹˜ ì ìš©
        if self.tau_s is not None:
            accept = max_score >= self.tau_s
        else:
            # ì„ê³„ì¹˜ ì—†ìœ¼ë©´ ëª¨ë‘ ìˆ˜ìš© (closed-set fallback)
            accept = torch.ones_like(max_score, dtype=torch.bool)
        
        # ğŸ‹ ë§ˆì§„ ê·œì¹™ ì ìš©
        if self.use_margin and scores.shape[1] > 1:
            margin = top2.values[:, 0] - top2.values[:, 1]
            margin_ok = margin >= self.tau_m
            accept = accept & margin_ok
        
        # ğŸ‹ ê±°ë¶€ëœ ìƒ˜í”Œì€ unknown_idë¡œ ì„¤ì •
        pred[~accept] = self.unknown_id
        
        return pred
    
    @torch.no_grad()
    def get_openset_scores(self, x):
        """
        ğŸ‹ ì˜¤í”ˆì…‹ ì ìˆ˜ ìƒì„¸ ì •ë³´ ë°˜í™˜ (ë””ë²„ê¹…ìš©)
        
        Returns:
            dict with 'scores', 'predictions', 'margins', 'accept_mask'
        """
        # ğŸ„ NCMì´ ë¹„ì–´ìˆìœ¼ë©´ ë¹ˆ ê²°ê³¼ ë°˜í™˜
        if len(self.class_means_dict) == 0:  # ğŸ„
            return {  # ğŸ„
                'scores': torch.zeros((x.shape[0], 0), device=x.device),  # ğŸ„
                'top_scores': torch.zeros(x.shape[0], device=x.device),  # ğŸ„
                'predictions': torch.full((x.shape[0],), -1, dtype=torch.long, device=x.device),  # ğŸ„
                'margins': None,  # ğŸ„
                'accept_mask': torch.zeros(x.shape[0], dtype=torch.bool, device=x.device),  # ğŸ„
                'tau_s': self.tau_s,  # ğŸ„
                'tau_m': self.tau_m if self.use_margin else None  # ğŸ„
            }  # ğŸ„
        
        scores = self.forward(x)
        top2 = scores.topk(2, dim=1)
        
        margin = None
        if scores.shape[1] > 1:
            margin = top2.values[:, 0] - top2.values[:, 1]
        
        pred = self.predict_openset(x)
        accept_mask = pred != self.unknown_id
        
        return {
            'scores': scores,
            'top_scores': top2.values[:, 0],
            'predictions': pred,
            'margins': margin,
            'accept_mask': accept_mask,
            'tau_s': self.tau_s,
            'tau_m': self.tau_m if self.use_margin else None
        }
    
    @torch.no_grad()
    def verify_equivalence(self, x, tolerance=1e-5):
        """
        ğŸ§ª ìµœì í™” ë°©ì‹ê³¼ cdist ë°©ì‹ì˜ ì˜ˆì¸¡ ì¼ì¹˜ ê²€ì¦
        """
        # ìµœì í™” ë°©ì‹
        pred_opt = self.forward(x).argmax(dim=1)
        
        # cdist ë°©ì‹
        pred_cdist = self.forward_cdist(x).argmax(dim=1)
        
        # ì˜ˆì¸¡ ì¼ì¹˜ìœ¨ë§Œ í™•ì¸ (ì ìˆ˜ ìŠ¤ì¼€ì¼ì€ ë‹¤ë¥¼ ìˆ˜ ìˆìŒ)
        accuracy = (pred_opt == pred_cdist).float().mean()
        
        print(f"ğŸ§ª ê²€ì¦ ê²°ê³¼: ì˜ˆì¸¡ ì¼ì¹˜ìœ¨ {accuracy*100:.2f}%")
        
        return accuracy == 1.0


# í…ŒìŠ¤íŠ¸ ì½”ë“œ
if __name__ == "__main__":
    import time
    
    print("=== ì½”ì‚¬ì¸ NCM í…ŒìŠ¤íŠ¸ ===")
    ncm_cos = NCMClassifier(normalize=True)
    
    # ğŸ„ ë¹ˆ NCM í…ŒìŠ¤íŠ¸
    print("\nğŸ„ ë¹ˆ NCM í…ŒìŠ¤íŠ¸:")
    x_test = torch.randn(10, 128)
    pred_empty = ncm_cos.predict(x_test)
    print(f"ë¹ˆ NCM ì˜ˆì¸¡: {pred_empty} (ëª¨ë‘ -1ì´ì–´ì•¼ í•¨)")
    
    # ì •ê·œí™”ëœ í”„ë¡œí† íƒ€ì…
    class_means = {
        0: F.normalize(torch.randn(128), p=2, dim=0),
        1: F.normalize(torch.randn(128), p=2, dim=0),
    }
    ncm_cos.replace_class_means_dict(class_means)
    
    x = torch.randn(100, 128)
    
    # ì†ë„ í…ŒìŠ¤íŠ¸
    start = time.time()
    for _ in range(100):
        _ = ncm_cos.predict(x)
    print(f"ì½”ì‚¬ì¸ NCM: {time.time()-start:.3f}ì´ˆ")
    
    # ë™ì¼ì„± ê²€ì¦
    ncm_cos.verify_equivalence(x)
    
    # ğŸ‹ === ì˜¤í”ˆì…‹ í…ŒìŠ¤íŠ¸ ì¶”ê°€ ===
    print("\n=== ì˜¤í”ˆì…‹ í…ŒìŠ¤íŠ¸ ===")
    
    # ì„ê³„ì¹˜ ì„¤ì •
    ncm_cos.set_thresholds(tau_s=0.7, use_margin=True, tau_m=0.05)
    
    # ì˜¤í”ˆì…‹ ì˜ˆì¸¡
    pred_openset = ncm_cos.predict_openset(x)
    rejected = (pred_openset == -1).sum().item()
    print(f"ì˜¤í”ˆì…‹ ì˜ˆì¸¡: {rejected}/{len(x)} ìƒ˜í”Œ ê±°ë¶€ë¨")
    
    # ìƒì„¸ ì •ë³´
    details = ncm_cos.get_openset_scores(x)
    print(f"í‰ê·  ìµœê³  ì ìˆ˜: {details['top_scores'].mean():.3f}")
    if details['margins'] is not None:
        print(f"í‰ê·  ë§ˆì§„: {details['margins'].mean():.3f}")