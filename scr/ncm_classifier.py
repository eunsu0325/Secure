# scr/ncm_classifier.py
import numpy as np  # â­ï¸ ì—ë„ˆì§€ ìŠ¤ì½”ì–´ë¥¼ ìœ„í•´ ì¶”ê°€
from typing import Dict
import torch
from torch import Tensor, nn
import torch.nn.functional as F


class NCMClassifier(nn.Module):
    """
    NCM (Nearest Class Mean) Classifier - ìµœì í™” ë²„ì „.
    
    ğŸš€ ìµœì í™”: cdist ëŒ€ì‹  í–‰ë ¬ê³± ì‚¬ìš© (2-3ë°° ë¹ ë¦„)
    âœ… normalize=True: ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê¸°ë°˜
    â­ï¸ ì—ë„ˆì§€ ìŠ¤ì½”ì–´ ê¸°ë°˜ ì˜¤í”ˆì…‹ ì§€ì›
    """

    def __init__(self, normalize: bool = True):
        super().__init__()
        self.register_buffer("class_means", None)
        self.class_means_dict = {}
        self.normalize = normalize
        self.max_class = -1
        
        # ì˜¤í”ˆì…‹ ê´€ë ¨
        self.tau_s = None            # ì „ì—­ ì„ê³„ì¹˜
        # âš¡ï¸ self.use_margin = False  # ì œê±°: ì—ë„ˆì§€ ìŠ¤ì½”ì–´ë¡œ ëŒ€ì²´
        # âš¡ï¸ self.tau_m = 0.05        # ì œê±°: ì—ë„ˆì§€ ìŠ¤ì½”ì–´ë¡œ ëŒ€ì²´
        self.unknown_id = -1        # Unknown í´ë˜ìŠ¤ ID
        
        # âš¡ï¸ Z-norm ê´€ë ¨ ì™„ì „ ì œê±° (ë³µì¡ì„± ëŒ€ë¹„ íš¨ê³¼ ë¯¸ë¯¸)
        # self.use_znorm = False
        # self.impostor_stats = {}
        
        # â­ï¸ ì—ë„ˆì§€ ìŠ¤ì½”ì–´ ì„¤ì • ì¶”ê°€
        self.use_energy = False      # ê¸°ë³¸ê°’ False (ê¸°ì¡´ ì½”ë“œ í˜¸í™˜ì„±)
        self.energy_T = 0.15         # Temperature (SupCon 0.07ë³´ë‹¤ ë†’ê²Œ)
        self.energy_k_mode = 'sqrt'  # 'sqrt', 'fixed', 'log'
        self.energy_k_fixed = 10     # k_mode='fixed'ì¼ ë•Œ ì‚¬ìš©

    def load_state_dict(self, state_dict, strict: bool = True):
        """ì²´í¬í¬ì¸íŠ¸ì—ì„œ ìƒíƒœë¥¼ ë¡œë“œí•©ë‹ˆë‹¤."""
        self.class_means = state_dict["class_means"]
        super().load_state_dict(state_dict, strict)
        if self.class_means is not None:
            for i in range(self.class_means.shape[0]):
                if (self.class_means[i] != 0).any():
                    self.class_means_dict[i] = self.class_means[i].clone()
        self.max_class = max(self.class_means_dict.keys()) if self.class_means_dict else -1

    def _vectorize_means_dict(self):
        """ë”•ì…”ë„ˆë¦¬ë¥¼ í…ì„œë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
        if self.class_means_dict == {}:
            return

        max_class = max(self.class_means_dict.keys())
        self.max_class = max(max_class, self.max_class)
        
        first_mean = list(self.class_means_dict.values())[0]
        feature_size = first_mean.size(0)
        device = first_mean.device
        
        self.class_means = torch.zeros(self.max_class + 1, feature_size).to(device)
        
        for k, v in self.class_means_dict.items():
            self.class_means[k] = self.class_means_dict[k].clone()

    @torch.no_grad()
    def forward(self, x):
        """
        ğŸš€ ìµœì í™”ëœ NCM ë¶„ë¥˜
        
        normalize=True: ì½”ì‚¬ì¸ ìœ ì‚¬ë„ (ì •ê·œí™” í›„ ë‚´ì )
        normalize=False: ìœ í´ë¦¬ë””ì•ˆ ê±°ë¦¬ (ì œê³± ê±°ë¦¬ ì‚¬ìš©)
        """
        # NCMì´ ë¹„ì–´ìˆìœ¼ë©´ ë¹ˆ ì ìˆ˜ ë°˜í™˜
        if self.class_means_dict == {}:
            return torch.zeros((x.shape[0], 0), device=x.device, dtype=x.dtype)
        
        # dtype ì¼ì¹˜ ë³´ì¥ (fp16/AMP ì§€ì›)
        M = self.class_means.to(device=x.device, dtype=x.dtype)
        
        if self.normalize:
            # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê¸°ë°˜
            x = F.normalize(x, p=2, dim=1, eps=1e-12)
            scores = x @ M.T  # (B, C)
            return scores  # ë†’ì„ìˆ˜ë¡ ê°€ê¹Œì›€
        else:
            # ìœ í´ë¦¬ë””ì•ˆ ê±°ë¦¬ ê¸°ë°˜
            x2 = (x * x).sum(dim=1, keepdim=True)      # (B, 1)
            m2 = (M * M).sum(dim=1, keepdim=False)      # (C,)
            xm = x @ M.T                                # (B, C)
            scores = -(x2 + m2.unsqueeze(0) - 2 * xm)  # (B, C)
            return scores  # ë†’ì„ìˆ˜ë¡ ê°€ê¹Œì›€ (negative distance)

    # âš¡ï¸ forward_cdist ì œê±° (í…ŒìŠ¤íŠ¸ìš©ì´ì—ˆìœ¼ë‚˜ ë” ì´ìƒ ë¶ˆí•„ìš”)
    # âš¡ï¸ 90-105 ë¼ì¸ ì „ì²´ ì œê±°

    def update_class_means_dict(self, class_means_dict: Dict[int, Tensor], momentum: float = 0.5):
        """
        í´ë˜ìŠ¤ í‰ê· ì„ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤. (EMA ì§€ì›)
        ë‚˜ì¤‘ì— EMA ì‹¤í—˜ì„ ìœ„í•´ ìœ ì§€
        """
        assert 0 <= momentum <= 1
        assert isinstance(class_means_dict, dict)
        
        for k, v in class_means_dict.items():
            if k not in self.class_means_dict or (self.class_means_dict[k] == 0).all():
                # ìƒˆë¡œìš´ í´ë˜ìŠ¤
                self.class_means_dict[k] = class_means_dict[k].clone()
            else:
                # ê¸°ì¡´ í´ë˜ìŠ¤ ì—…ë°ì´íŠ¸
                device = self.class_means_dict[k].device
                self.class_means_dict[k] = (
                    momentum * class_means_dict[k].to(device)
                    + (1 - momentum) * self.class_means_dict[k]
                )
        
        # ë°©ì–´ì  ì •ê·œí™” (normalize=Trueì¼ ë•Œ)
        if self.normalize:
            for k in self.class_means_dict:
                self.class_means_dict[k] = F.normalize(
                    self.class_means_dict[k], p=2, dim=0, eps=1e-12
                )
        
        self._vectorize_means_dict()

    def replace_class_means_dict(self, class_means_dict: Dict[int, Tensor]):
        """
        ê¸°ì¡´ í‰ê· ì„ ì™„ì „íˆ êµì²´í•©ë‹ˆë‹¤.
        í˜„ì¬ ì£¼ë¡œ ì‚¬ìš©ë˜ëŠ” ë©”ì„œë“œ
        """
        assert isinstance(class_means_dict, dict)
        
        self.class_means_dict = {k: v.clone() for k, v in class_means_dict.items()}
        
        # ë°©ì–´ì  ì •ê·œí™” (normalize=Trueì¼ ë•Œ)
        if self.normalize:
            for k in self.class_means_dict:
                self.class_means_dict[k] = F.normalize(
                    self.class_means_dict[k], p=2, dim=0, eps=1e-12
                )
        
        self._vectorize_means_dict()

    # âš¡ï¸ init_missing_classes ì œê±° (ì‚¬ìš©í•˜ì§€ ì•ŠìŒ)
    # âš¡ï¸ adaptation ì œê±° (Avalanche ì „ìš©)
    
    def predict(self, x):
        """í´ë˜ìŠ¤ ì˜ˆì¸¡ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
        # NCMì´ ë¹„ì–´ìˆìœ¼ë©´ -1 ë°˜í™˜
        if len(self.class_means_dict) == 0:
            return torch.full((x.shape[0],), -1, dtype=torch.long, device=x.device)
        
        scores = self.forward(x)
        return scores.argmax(dim=1)
    
    def get_num_classes(self):
        """í˜„ì¬ ì €ì¥ëœ í´ë˜ìŠ¤ ìˆ˜ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
        return len(self.class_means_dict)
    
    def get_class_means(self):
        """í˜„ì¬ ì €ì¥ëœ í´ë˜ìŠ¤ í‰ê· ë“¤ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
        return self.class_means_dict.copy()
    
    # â­ï¸ === ì—ë„ˆì§€ ìŠ¤ì½”ì–´ ë©”ì„œë“œ ì¶”ê°€ ===
    
    @torch.no_grad()
    def compute_energy_score(self, x, k=None, T=None):
        """
        â­ï¸ ì—ë„ˆì§€ ê¸°ë°˜ ê²Œì´íŠ¸ ìŠ¤ì½”ì–´ ê³„ì‚°
        
        Args:
            x: (B, D) íŠ¹ì§• ë²¡í„°
            k: Top-k íŒŒë¼ë¯¸í„° (Noneì´ë©´ ìë™ ê²°ì •)
            T: Temperature (Noneì´ë©´ self.energy_T ì‚¬ìš©)
            
        Returns:
            (B,) ê²Œì´íŠ¸ ìŠ¤ì½”ì–´ (ë†’ì„ìˆ˜ë¡ ë“±ë¡ììŠ¤ëŸ¬ì›€)
        """
        # NCMì´ ë¹„ì–´ìˆìœ¼ë©´ ë‚®ì€ ì ìˆ˜ ë°˜í™˜
        if len(self.class_means_dict) == 0:
            return torch.full((x.shape[0],), -1000.0, device=x.device, dtype=x.dtype)
        
        # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
        scores = self.forward(x)  # (B, C), [-1, 1]
        B, C = scores.shape
        
        # k ê²°ì •
        if k is None:
            if self.energy_k_mode == 'sqrt':
                k = max(3, min(20, int(np.sqrt(C))))
            elif self.energy_k_mode == 'log':
                k = max(3, min(20, int(np.log(C) * 3)))
            else:  # fixed
                k = self.energy_k_fixed
        
        # Temperature ì„¤ì •
        T = T if T is not None else self.energy_T
        
        # kë¥¼ í´ë˜ìŠ¤ ìˆ˜ë¡œ ì œí•œ
        k_prime = min(k, C)
        
        # Top-k ì¶”ì¶œ
        topk_values, _ = scores.topk(k_prime, dim=1)  # (B, k')
        
        # ìˆ˜ì¹˜ ì•ˆì • LogSumExp
        z = topk_values / T  # (B, k')
        max_z = z.max(dim=1, keepdim=True).values  # (B, 1)
        
        # energy = T * log(sum(exp(s/T)))
        energy = T * (max_z.squeeze(1) + torch.log(torch.exp(z - max_z).sum(dim=1)))
        
        # k ë³´ì • (ì •ê·œí™”)
        gate_score = energy - T * np.log(k_prime)
        
        return gate_score
    
    @torch.no_grad()
    def compute_energy_masked(self, x, labels, k=None, T=None):
        """
        â­ï¸ ìê¸° í´ë˜ìŠ¤ë¥¼ ì œì™¸í•œ Top-k ì—ë„ˆì§€ ê³„ì‚° (Between impostorìš©)
        
        Args:
            x: (B, D) íŠ¹ì§• ë²¡í„°
            labels: (B,) ì •ë‹µ ë ˆì´ë¸”
            
        Returns:
            (B,) ë§ˆìŠ¤í‚¹ëœ ê²Œì´íŠ¸ ìŠ¤ì½”ì–´
        """
        if len(self.class_means_dict) == 0:
            return torch.full((x.shape[0],), -1000.0, device=x.device, dtype=x.dtype)
        
        scores = self.forward(x)  # (B, C)
        B, C = scores.shape
        
        # k ê²°ì •
        if k is None:
            if self.energy_k_mode == 'sqrt':
                k = max(3, min(20, int(np.sqrt(C))))
            else:
                k = self.energy_k_fixed
        
        T = T if T is not None else self.energy_T
        
        # ìê¸° í´ë˜ìŠ¤ ì œì™¸í•˜ë¯€ë¡œ C-1ì´ ìµœëŒ€
        k_prime = max(1, min(k, C - 1))
        
        # ìê¸° í´ë˜ìŠ¤ ë§ˆìŠ¤í‚¹
        mask = F.one_hot(labels, num_classes=C).bool()  # (B, C)
        scores_masked = scores.masked_fill(mask, float('-inf'))  # (B, C)
        
        # Top-k ì¶”ì¶œ (ìê¸° ì œì™¸)
        topk_values, _ = scores_masked.topk(k_prime, dim=1)  # (B, k')
        
        # ìˆ˜ì¹˜ ì•ˆì • LogSumExp
        z = topk_values / T
        max_z = z.max(dim=1, keepdim=True).values
        energy = T * (max_z.squeeze(1) + torch.log(torch.exp(z - max_z).sum(dim=1)))
        
        # k ë³´ì •
        gate_score = energy - T * np.log(k_prime)
        
        return gate_score
    
    # === ì˜¤í”ˆì…‹ ê´€ë ¨ ë©”ì„œë“œ (ìˆ˜ì •) ===
    
    def set_thresholds(self, tau_s: float):
        """
        ì˜¤í”ˆì…‹ ì„ê³„ì¹˜ ì„¤ì • (ì‹¬í”Œí™”)
        âš¡ï¸ ë§ˆì§„ íŒŒë¼ë¯¸í„° ì œê±°
        """
        self.tau_s = float(tau_s)
        # âš¡ï¸ use_margin, tau_m ê´€ë ¨ ì œê±°
    
    # âš¡ï¸ enable_znorm ì œê±° (Z-norm ë¯¸ì‚¬ìš©)
    # âš¡ï¸ update_impostor_stats ì œê±° (Z-norm ë¯¸ì‚¬ìš©)
    
    def set_energy_config(self, use_energy=True, T=0.15, k_mode='sqrt', k_fixed=10):
        """
        â­ï¸ ì—ë„ˆì§€ ìŠ¤ì½”ì–´ ì„¤ì •
        """
        self.use_energy = use_energy
        self.energy_T = T
        self.energy_k_mode = k_mode
        self.energy_k_fixed = k_fixed
        
        if use_energy:
            print(f"âš¡ Energy mode enabled: T={T}, k_mode={k_mode}")
    
    @torch.no_grad()
    def predict_openset(self, x):
        """
        ì˜¤í”ˆì…‹ ì˜ˆì¸¡ (ì—ë„ˆì§€ ë˜ëŠ” ìµœëŒ“ê°’ ëª¨ë“œ)
        """
        # NCMì´ ë¹„ì–´ìˆìœ¼ë©´ ëª¨ë‘ -1 ë°˜í™˜
        if len(self.class_means_dict) == 0:
            return torch.full((x.shape[0],), -1, dtype=torch.long, device=x.device)
        
        # â­ï¸ ì—ë„ˆì§€ ëª¨ë“œ
        if self.use_energy:
            return self.predict_openset_energy(x)
        
        # ê¸°ì¡´ ìµœëŒ“ê°’ ëª¨ë“œ (Z-norm, ë§ˆì§„ ì œê±°)
        scores = self.forward(x)  # (B, C)
        
        # Top-1 ì¶”ì¶œ
        top1 = scores.topk(1, dim=1)
        max_score = top1.values[:, 0]
        pred = top1.indices[:, 0]
        
        # âš¡ï¸ Z-norm ë¡œì§ ì œê±° (241-251 ë¼ì¸)
        # âš¡ï¸ ë§ˆì§„ ë¡œì§ ì œê±° (261-264 ë¼ì¸)
        
        # ì„ê³„ì¹˜ ì ìš©
        if self.tau_s is not None:
            accept = max_score >= self.tau_s
        else:
            accept = torch.ones_like(max_score, dtype=torch.bool)
        
        pred[~accept] = self.unknown_id
        
        return pred
    
    @torch.no_grad()
    def predict_openset_energy(self, x):
        """
        â­ï¸ ì—ë„ˆì§€ ìŠ¤ì½”ì–´ ê¸°ë°˜ ì˜¤í”ˆì…‹ ì˜ˆì¸¡
        """
        # NCMì´ ë¹„ì–´ìˆìœ¼ë©´ ëª¨ë‘ ê±°ë¶€
        if len(self.class_means_dict) == 0:
            return torch.full((x.shape[0],), -1, dtype=torch.long, device=x.device)
        
        # ê²Œì´íŠ¸ ìŠ¤ì½”ì–´ ê³„ì‚°
        gate_scores = self.compute_energy_score(x)
        
        # ì„ê³„ì¹˜ ë¹„êµ
        if self.tau_s is not None:
            accept = gate_scores >= self.tau_s
        else:
            accept = torch.ones(gate_scores.shape[0], dtype=torch.bool, device=x.device)
        
        # í´ë˜ìŠ¤ ì˜ˆì¸¡ (argmaxëŠ” ê·¸ëŒ€ë¡œ)
        scores = self.forward(x)
        pred_classes = scores.argmax(dim=1)
        
        # ê±°ë¶€ëœ ìƒ˜í”Œì€ -1
        pred_classes[~accept] = self.unknown_id
        
        return pred_classes
    
    @torch.no_grad()
    def get_openset_scores(self, x):
        """
        ì˜¤í”ˆì…‹ ì ìˆ˜ ìƒì„¸ ì •ë³´ ë°˜í™˜ (ë””ë²„ê¹…ìš©)
        â­ï¸ ì—ë„ˆì§€ ìŠ¤ì½”ì–´ ì •ë³´ ì¶”ê°€
        """
        # NCMì´ ë¹„ì–´ìˆìœ¼ë©´ ë¹ˆ ê²°ê³¼ ë°˜í™˜
        if len(self.class_means_dict) == 0:
            return {
                'scores': torch.zeros((x.shape[0], 0), device=x.device),
                'top_scores': torch.zeros(x.shape[0], device=x.device),
                'gate_scores': torch.full((x.shape[0],), -1000.0, device=x.device) if self.use_energy else None,  # â­ï¸
                'predictions': torch.full((x.shape[0],), -1, dtype=torch.long, device=x.device),
                'accept_mask': torch.zeros(x.shape[0], dtype=torch.bool, device=x.device),
                'tau_s': self.tau_s,
                'mode': 'energy' if self.use_energy else 'max'  # â­ï¸
            }
        
        scores = self.forward(x)
        top1 = scores.topk(1, dim=1)
        
        # â­ï¸ ì—ë„ˆì§€ ìŠ¤ì½”ì–´ ì¶”ê°€
        gate_scores = None
        if self.use_energy:
            gate_scores = self.compute_energy_score(x)
        
        pred = self.predict_openset(x)
        accept_mask = pred != self.unknown_id
        
        return {
            'scores': scores,
            'top_scores': top1.values[:, 0],
            'gate_scores': gate_scores,  # â­ï¸
            'predictions': pred,
            'accept_mask': accept_mask,
            'tau_s': self.tau_s,
            'mode': 'energy' if self.use_energy else 'max'  # â­ï¸
        }
    
    # âš¡ï¸ verify_equivalence ì œê±° (í…ŒìŠ¤íŠ¸ìš©ì´ì—ˆìœ¼ë‚˜ ë¶ˆí•„ìš”)


# í…ŒìŠ¤íŠ¸ ì½”ë“œ
if __name__ == "__main__":
    import time
    
    print("=== ì½”ì‚¬ì¸ NCM í…ŒìŠ¤íŠ¸ ===")
    ncm_cos = NCMClassifier(normalize=True)
    
    # ë¹ˆ NCM í…ŒìŠ¤íŠ¸
    print("\në¹ˆ NCM í…ŒìŠ¤íŠ¸:")
    x_test = torch.randn(10, 128)
    pred_empty = ncm_cos.predict(x_test)
    print(f"ë¹ˆ NCM ì˜ˆì¸¡: {pred_empty} (ëª¨ë‘ -1ì´ì–´ì•¼ í•¨)")
    
    # ì •ê·œí™”ëœ í”„ë¡œí† íƒ€ì… ìƒì„±
    class_means = {}
    for i in range(10):
        class_means[i] = F.normalize(torch.randn(128), p=2, dim=0)
    ncm_cos.replace_class_means_dict(class_means)
    
    x = torch.randn(100, 128)
    
    # â­ï¸ === ì—ë„ˆì§€ ìŠ¤ì½”ì–´ í…ŒìŠ¤íŠ¸ ===
    print("\n=== ì—ë„ˆì§€ ìŠ¤ì½”ì–´ í…ŒìŠ¤íŠ¸ ===")
    
    # ì—ë„ˆì§€ ëª¨ë“œ í™œì„±í™”
    ncm_cos.set_energy_config(use_energy=True, T=0.15, k_mode='sqrt')
    ncm_cos.set_thresholds(tau_s=0.0)
    
    # ê²Œì´íŠ¸ ìŠ¤ì½”ì–´ ê³„ì‚°
    gate_scores = ncm_cos.compute_energy_score(x)
    print(f"Gate scores: min={gate_scores.min():.3f}, max={gate_scores.max():.3f}, mean={gate_scores.mean():.3f}")
    
    # ëª¨ë“œ ë¹„êµ
    ncm_cos.use_energy = False
    pred_max = ncm_cos.predict_openset(x)
    
    ncm_cos.use_energy = True
    pred_energy = ncm_cos.predict_openset(x)
    
    print(f"Max mode rejections: {(pred_max == -1).sum()}/{len(x)}")
    print(f"Energy mode rejections: {(pred_energy == -1).sum()}/{len(x)}")
    
    # ìê¸° í´ë˜ìŠ¤ ë§ˆìŠ¤í‚¹ í…ŒìŠ¤íŠ¸
    labels = torch.randint(0, 10, (20,))
    x_test = torch.randn(20, 128)
    masked_scores = ncm_cos.compute_energy_masked(x_test, labels)
    print(f"Masked gate scores: mean={masked_scores.mean():.3f}")
    
    # ìƒì„¸ ì •ë³´
    details = ncm_cos.get_openset_scores(x)
    print(f"Mode: {details['mode']}, Tau_s: {details['tau_s']}")