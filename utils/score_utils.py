# ğŸ‹ scr/score_utils.py (ì‹ ê·œ - ìˆ˜ì • ë²„ì „)
"""
ì˜¤í”ˆì…‹ EER ê³„ì‚°ì„ ìœ„í•œ ì ìˆ˜ ì¶”ì¶œ ìœ í‹¸ë¦¬í‹°
- genuine scores: ê°™ì€ í´ë˜ìŠ¤ ì ìˆ˜
- impostor scores: ë‹¤ë¥¸ í´ë˜ìŠ¤/unknown ì ìˆ˜
"""

import random
import numpy as np
import torch
import torch.nn.functional as F
from torch.utils.data import DataLoader, Dataset, Subset
from pathlib import Path
from PIL import Image
from typing import Optional, List, Tuple, Dict, Set
from collections import defaultdict


def to_loader(dataset, batch_size=128, num_workers=0):
    """ğŸ‹ ë°ì´í„°ì…‹ì„ DataLoaderë¡œ ë³€í™˜"""
    return DataLoader(
        dataset, 
        batch_size=batch_size, 
        shuffle=False, 
        num_workers=num_workers,
        pin_memory=torch.cuda.is_available()
    )


def load_txt_dataset(txt_path: str) -> Tuple[List[str], List[int]]:
    """
    ğŸ‹ txt íŒŒì¼ì—ì„œ ì´ë¯¸ì§€ ê²½ë¡œì™€ ë ˆì´ë¸” ì½ê¸°
    
    Args:
        txt_path: train_palma.txt ë˜ëŠ” train_Tongji.txt ê²½ë¡œ
        
    Returns:
        (paths, labels) íŠœí”Œ
    """
    paths = []
    labels = []
    
    if not Path(txt_path).exists():
        print(f"âš ï¸ File not found: {txt_path}")
        return paths, labels
    
    with open(txt_path, 'r') as f:
        for line in f:
            parts = line.strip().split(' ')
            if len(parts) == 2:
                path, label = parts
                paths.append(path)
                labels.append(int(label))
    
    print(f"ğŸ“‚ Loaded {len(paths)} samples from {Path(txt_path).name}")
    return paths, labels


class SimpleImageDataset(Dataset):
    """ğŸ‹ ê°„ë‹¨í•œ ì´ë¯¸ì§€ ë°ì´í„°ì…‹ (ê²½ë¡œ ë¦¬ìŠ¤íŠ¸ë¡œë¶€í„°)"""
    
    def __init__(self, paths: List[str], labels: List[int], transform=None):
        self.paths = paths
        self.labels = labels
        self.transform = transform
        
    def __len__(self):
        return len(self.paths)
    
    def __getitem__(self, idx):
        img = Image.open(self.paths[idx]).convert('L')
        
        if self.transform:
            img = self.transform(img)
            
        return img, self.labels[idx]


@torch.no_grad()
def extract_scores_genuine(model, ncm, known_dev_paths: List[str], 
                          known_dev_labels: List[int], transform, device, 
                          batch_size=128) -> np.ndarray:
    """
    ğŸ‹ Known-Dev ìƒ˜í”Œì˜ 'ìê¸° í´ë˜ìŠ¤' ì ìˆ˜ s_genuine ìˆ˜ì§‘
    
    Args:
        model: íŠ¹ì§• ì¶”ì¶œ ëª¨ë¸
        ncm: NCM classifier
        known_dev_paths: Dev ìƒ˜í”Œ ê²½ë¡œë“¤
        known_dev_labels: Dev ìƒ˜í”Œ ë ˆì´ë¸”ë“¤
        transform: ì´ë¯¸ì§€ ì „ì²˜ë¦¬
        device: cuda/cpu
        
    Returns:
        np.array: genuine ì ìˆ˜ë“¤ (ì½”ì‚¬ì¸ ìœ ì‚¬ë„)
    """
    if len(known_dev_paths) == 0:
        return np.array([])
    
    model.eval()
    scores = []
    
    # ë°ì´í„°ì…‹ ìƒì„±
    dataset = SimpleImageDataset(known_dev_paths, known_dev_labels, transform)
    loader = to_loader(dataset, batch_size=batch_size)
    
    with torch.no_grad():
        for data, labels in loader:
            x = data.to(device)
            y = labels.to(device)
            
            # íŠ¹ì§• ì¶”ì¶œ
            fe = model.getFeatureCode(x)  # (B, D), normalized if cosine
            
            # NCM ì ìˆ˜ ê³„ì‚°
            if ncm.class_means is None or ncm.class_means.shape[0] == 0:
                continue
                
            S = fe @ ncm.class_means.to(device).T  # (B, C) ì½”ì‚¬ì¸ ìœ ì‚¬ë„
            
            # ìê¸° í´ë˜ìŠ¤ ì ìˆ˜ë§Œ ì¶”ì¶œ
            batch_size = len(y)
            for i in range(batch_size):
                class_id = y[i].item()
                # í´ë˜ìŠ¤ê°€ NCMì— ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
                if class_id < S.shape[1] and class_id in ncm.class_means_dict:
                    score = S[i, class_id].item()
                    scores.append(score)
    
    return np.array(scores)


@torch.no_grad()
def extract_scores_impostor_between_registered(model, ncm, known_dev_paths: List[str],
                                              known_dev_labels: List[int], transform, 
                                              device, max_pairs=2000) -> np.ndarray:
    """
    ğŸ‹ ë“±ë¡ëœ í´ë˜ìŠ¤ ê°„ impostor ì ìˆ˜ (ë‹¤ë¥¸ í´ë˜ìŠ¤ì— ëŒ€í•œ ì ìˆ˜)
    
    Returns:
        np.array: ë“±ë¡ ì‚¬ìš©ì ê°„ impostor ì ìˆ˜ë“¤
    """
    if len(known_dev_paths) == 0:
        return np.array([])
    
    model.eval()
    scores = []
    
    # ë°ì´í„°ì…‹ ìƒì„±
    dataset = SimpleImageDataset(known_dev_paths, known_dev_labels, transform)
    loader = to_loader(dataset, batch_size=128)
    
    with torch.no_grad():
        for data, labels in loader:
            x = data.to(device)
            y = labels.to(device)
            
            # íŠ¹ì§• ì¶”ì¶œ
            fe = model.getFeatureCode(x)
            
            # NCM ì ìˆ˜ ê³„ì‚°
            if ncm.class_means is None or ncm.class_means.shape[0] == 0:
                continue
                
            S = fe @ ncm.class_means.to(device).T  # (B, C)
            
            # ë‹¤ë¥¸ í´ë˜ìŠ¤ë“¤ì— ëŒ€í•œ ì ìˆ˜ ìˆ˜ì§‘
            batch_size = len(y)
            for i in range(batch_size):
                true_class = y[i].item()
                
                # ìê¸° í´ë˜ìŠ¤ë¥¼ ì œì™¸í•œ ëª¨ë“  ë“±ë¡ í´ë˜ìŠ¤ì˜ ì ìˆ˜
                for class_id in ncm.class_means_dict.keys():
                    if class_id != true_class and class_id < S.shape[1]:
                        scores.append(S[i, class_id].item())
                        
                        if len(scores) >= max_pairs:
                            return np.array(scores)
    
    return np.array(scores)


@torch.no_grad()
def extract_scores_impostor_unknown(model, ncm, unknown_txt_path: str,
                                   registered_users: Set[int], transform,
                                   device, max_eval=5000) -> np.ndarray:
    """
    ğŸ‹ ì•„ì§ ë“±ë¡ ì•ˆ ëœ ì‚¬ìš©ìë“¤ì˜ impostor ì ìˆ˜
    
    Args:
        unknown_txt_path: ì „ì²´ ë°ì´í„°ì…‹ txt ê²½ë¡œ (train_palma.txt)
        registered_users: í˜„ì¬ ë“±ë¡ëœ ì‚¬ìš©ì ID ì§‘í•©
        
    Returns:
        np.array: ë¯¸ë“±ë¡ ì‚¬ìš©ìì˜ impostor ì ìˆ˜ë“¤
    """
    # ì „ì²´ ë°ì´í„° ì½ê¸°
    all_paths, all_labels = load_txt_dataset(unknown_txt_path)
    
    if len(all_paths) == 0:
        return np.array([])
    
    # ë“±ë¡ ì•ˆ ëœ ì‚¬ìš©ìë§Œ í•„í„°ë§
    unknown_paths = []
    unknown_labels = []
    
    for path, label in zip(all_paths, all_labels):
        if label not in registered_users:
            unknown_paths.append(path)
            unknown_labels.append(label)
    
    if len(unknown_paths) == 0:
        print("âš ï¸ No unknown users found")
        return np.array([])
    
    # ìƒ˜í”Œë§ (ë„ˆë¬´ ë§ìœ¼ë©´)
    if max_eval and len(unknown_paths) > max_eval:
        indices = random.sample(range(len(unknown_paths)), max_eval)
        unknown_paths = [unknown_paths[i] for i in indices]
        unknown_labels = [unknown_labels[i] for i in indices]
    
    print(f"ğŸ“Š Using {len(unknown_paths)} unknown samples for impostor scores")
    
    model.eval()
    scores = []
    
    # ë°ì´í„°ì…‹ ìƒì„±
    dataset = SimpleImageDataset(unknown_paths, unknown_labels, transform)
    loader = to_loader(dataset, batch_size=128)
    
    with torch.no_grad():
        for data, labels in loader:
            x = data.to(device)
            
            # íŠ¹ì§• ì¶”ì¶œ
            fe = model.getFeatureCode(x)
            
            # NCM ì ìˆ˜ ê³„ì‚°
            if ncm.class_means is None or ncm.class_means.shape[0] == 0:
                continue
                
            S = fe @ ncm.class_means.to(device).T  # (B, C)
            
            # ìµœê³  ì ìˆ˜ (ê°€ì¥ ê°€ê¹Œìš´ ë“±ë¡ í´ë˜ìŠ¤)
            max_scores = S.max(dim=1).values  # (B,)
            scores.extend(max_scores.cpu().numpy().tolist())
    
    return np.array(scores)


@torch.no_grad()
def extract_scores_impostor_negref(model, ncm, negref_txt_path: str,
                                  transform, device, max_eval=5000) -> np.ndarray:
    """
    ğŸ‹ Negative Reference (train_Tongji.txt)ì˜ impostor ì ìˆ˜
    
    Args:
        negref_txt_path: negative_samples_file ê²½ë¡œ
        
    Returns:
        np.array: negative reference impostor ì ìˆ˜ë“¤
    """
    # Negative ë°ì´í„° ì½ê¸°
    neg_paths, neg_labels = load_txt_dataset(negref_txt_path)
    
    if len(neg_paths) == 0:
        return np.array([])
    
    # ìƒ˜í”Œë§
    if max_eval and len(neg_paths) > max_eval:
        indices = random.sample(range(len(neg_paths)), max_eval)
        neg_paths = [neg_paths[i] for i in indices]
        neg_labels = [neg_labels[i] for i in indices]
    
    print(f"ğŸ“Š Using {len(neg_paths)} NegRef samples for impostor scores")
    
    model.eval()
    scores = []
    
    # ë°ì´í„°ì…‹ ìƒì„±
    dataset = SimpleImageDataset(neg_paths, neg_labels, transform)
    loader = to_loader(dataset, batch_size=128)
    
    with torch.no_grad():
        for data, labels in loader:
            x = data.to(device)
            
            # íŠ¹ì§• ì¶”ì¶œ
            fe = model.getFeatureCode(x)
            
            # NCM ì ìˆ˜ ê³„ì‚°
            if ncm.class_means is None or ncm.class_means.shape[0] == 0:
                continue
                
            S = fe @ ncm.class_means.to(device).T  # (B, C)
            
            # ìµœê³  ì ìˆ˜ (ê°€ì¥ ê°€ê¹Œìš´ ë“±ë¡ í´ë˜ìŠ¤)
            max_scores = S.max(dim=1).values  # (B,)
            scores.extend(max_scores.cpu().numpy().tolist())
    
    return np.array(scores)


def balance_impostor_scores(scores_between: np.ndarray,
                           scores_unknown: np.ndarray, 
                           scores_negref: np.ndarray,
                           ratio: Tuple[float, float, float] = (0.25, 0.25, 0.5)) -> np.ndarray:
    """
    ğŸ‹ ì„¸ ì¢…ë¥˜ì˜ impostor ì ìˆ˜ë¥¼ ê· í˜•ìˆê²Œ ì„ê¸°
    
    Args:
        scores_between: ë“±ë¡ í´ë˜ìŠ¤ ê°„ impostor
        scores_unknown: ë¯¸ë“±ë¡ ì‚¬ìš©ì impostor
        scores_negref: NegRef impostor
        ratio: (between, unknown, negref) ë¹„ìœ¨
        
    Returns:
        np.array: ê· í˜•ì¡íŒ impostor ì ìˆ˜
    """
    available = []
    if len(scores_between) > 0:
        available.append(('between', scores_between))
    if len(scores_unknown) > 0:
        available.append(('unknown', scores_unknown))
    if len(scores_negref) > 0:
        available.append(('negref', scores_negref))
    
    if len(available) == 0:
        return np.array([])
    
    # ğŸ‹ ê°€ìš©í•œ ì†ŒìŠ¤ì— ë”°ë¼ ë¹„ìœ¨ ì¬ì¡°ì •
    if len(available) == 1:
        return available[0][1]
    
    elif len(available) == 2:
        # ë‘ ê°œë§Œ ìˆìœ¼ë©´ 50:50
        source1, scores1 = available[0]
        source2, scores2 = available[1]
        
        min_size = min(len(scores1), len(scores2))
        sampled1 = np.random.choice(scores1, min_size, replace=False)
        sampled2 = np.random.choice(scores2, min_size, replace=False)
        
        balanced = np.concatenate([sampled1, sampled2])
        print(f"ğŸ“Š Balanced impostor: {source1}({min_size}) + {source2}({min_size})")
        
    else:
        # ì„¸ ê°œ ëª¨ë‘ ìˆìœ¼ë©´ ratio ì‚¬ìš©
        total_size = min(
            sum(len(s) for _, s in available),
            10000  # ìµœëŒ€ 10000ê°œ
        )
        
        n_between = int(total_size * ratio[0])
        n_unknown = int(total_size * ratio[1])  
        n_negref = int(total_size * ratio[2])
        
        # ì‹¤ì œ ê°€ìš© ê°œìˆ˜ë¡œ ì¡°ì •
        n_between = min(n_between, len(scores_between))
        n_unknown = min(n_unknown, len(scores_unknown))
        n_negref = min(n_negref, len(scores_negref))
        
        sampled = []
        if n_between > 0:
            sampled.append(np.random.choice(scores_between, n_between, replace=False))
        if n_unknown > 0:
            sampled.append(np.random.choice(scores_unknown, n_unknown, replace=False))
        if n_negref > 0:
            sampled.append(np.random.choice(scores_negref, n_negref, replace=False))
        
        balanced = np.concatenate(sampled)
        print(f"ğŸ“Š Balanced impostor: between({n_between}) + unknown({n_unknown}) + negref({n_negref})")
    
    return balanced


def compute_score_statistics(genuine_scores: np.ndarray, 
                            impostor_scores: np.ndarray) -> dict:
    """
    ğŸ‹ ì ìˆ˜ ë¶„í¬ í†µê³„ ê³„ì‚°
    
    Returns:
        dict: í‰ê· , í‘œì¤€í¸ì°¨, ë¶„ë¦¬ë„ ë“±
    """
    stats = {
        'genuine_mean': float(np.mean(genuine_scores)) if len(genuine_scores) > 0 else 0,
        'genuine_std': float(np.std(genuine_scores)) if len(genuine_scores) > 0 else 0,
        'genuine_min': float(np.min(genuine_scores)) if len(genuine_scores) > 0 else 0,
        'genuine_max': float(np.max(genuine_scores)) if len(genuine_scores) > 0 else 0,
        'genuine_count': len(genuine_scores),
        
        'impostor_mean': float(np.mean(impostor_scores)) if len(impostor_scores) > 0 else 0,
        'impostor_std': float(np.std(impostor_scores)) if len(impostor_scores) > 0 else 0,
        'impostor_min': float(np.min(impostor_scores)) if len(impostor_scores) > 0 else 0,
        'impostor_max': float(np.max(impostor_scores)) if len(impostor_scores) > 0 else 0,
        'impostor_count': len(impostor_scores),
    }
    
    # ğŸ‹ ë¶„ë¦¬ë„ (í´ìˆ˜ë¡ ì¢‹ìŒ)
    if len(genuine_scores) > 0 and len(impostor_scores) > 0:
        stats['separation'] = stats['genuine_mean'] - stats['impostor_mean']
        # D-prime (ì‹ í˜¸ ê²€ì¶œ ì´ë¡ )
        pooled_std = np.sqrt((stats['genuine_std']**2 + stats['impostor_std']**2) / 2 + 1e-6)
        stats['d_prime'] = stats['separation'] / pooled_std
    else:
        stats['separation'] = 0
        stats['d_prime'] = 0
    
    return stats


# ğŸ‹ Dev/Train ë¶„ë¦¬ í—¬í¼ í•¨ìˆ˜
def split_user_data(user_paths: List[str], user_labels: List[int], 
                   dev_ratio: float = 0.2) -> Tuple[List, List, List, List]:
    """
    ğŸ‹ ì‚¬ìš©ì ë°ì´í„°ë¥¼ Train/Devë¡œ ë¶„ë¦¬
    
    Returns:
        (train_paths, train_labels, dev_paths, dev_labels)
    """
    n = len(user_paths)
    indices = np.random.permutation(n)
    n_dev = max(2, int(n * dev_ratio))  # ìµœì†Œ 2ê°œëŠ” Dev
    
    dev_indices = indices[:n_dev]
    train_indices = indices[n_dev:]
    
    dev_paths = [user_paths[i] for i in dev_indices]
    dev_labels = [user_labels[i] for i in dev_indices]
    train_paths = [user_paths[i] for i in train_indices]
    train_labels = [user_labels[i] for i in train_indices]
    
    return train_paths, train_labels, dev_paths, dev_labels


# ğŸ‹ í…ŒìŠ¤íŠ¸ ì½”ë“œ
if __name__ == "__main__":
    print("=== Score Utils Test ===")
    
    # ê°€ì§œ ë°ì´í„°ë¡œ í…ŒìŠ¤íŠ¸
    genuine = np.random.normal(0.8, 0.1, 1000)  # ë†’ì€ ì ìˆ˜
    impostor = np.random.normal(0.3, 0.15, 1000)  # ë‚®ì€ ì ìˆ˜
    
    stats = compute_score_statistics(genuine, impostor)
    print(f"Genuine: {stats['genuine_mean']:.3f} Â± {stats['genuine_std']:.3f}")
    print(f"Impostor: {stats['impostor_mean']:.3f} Â± {stats['impostor_std']:.3f}")
    print(f"Separation: {stats['separation']:.3f}")
    print(f"D-prime: {stats['d_prime']:.3f}")
    
    # ê· í˜• í…ŒìŠ¤íŠ¸
    between_scores = np.random.normal(0.5, 0.1, 500)
    unknown_scores = np.random.normal(0.4, 0.1, 800)
    negref_scores = np.random.normal(0.2, 0.1, 1500)
    
    balanced = balance_impostor_scores(between_scores, unknown_scores, negref_scores)
    print(f"\nBalanced shape: {balanced.shape}")