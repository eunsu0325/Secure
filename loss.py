"""
Author: Yonglong Tian (yonglong@mit.edu)
Date: May 07, 2020
"""
from __future__ import print_function

import torch
import torch.nn as nn


class SupConLoss(nn.Module):
    """Supervised Contrastive Learning: https://arxiv.org/pdf/2004.11362.pdf.
    It also supports the unsupervised contrastive loss in SimCLR"""
    def __init__(self, temperature=0.07, contrast_mode='all',
                 base_temperature=0.07):
        super(SupConLoss, self).__init__()
        self.temperature = temperature
        self.contrast_mode = contrast_mode
        self.base_temperature = base_temperature

    def forward(self, features, labels=None, mask=None):
        """Compute loss for model. If both `labels` and `mask` are None,
        it degenerates to SimCLR unsupervised loss:
        https://arxiv.org/pdf/2002.05709.pdf

        Args:
            features: hidden vector of shape [bsz, n_views, ...].
            labels: ground truth of shape [bsz].
            mask: contrastive mask of shape [bsz, bsz], mask_{i,j}=1 if sample j
                has the same class as sample i. Can be asymmetric.
        Returns:
            A loss scalar.
        """
        device = (torch.device('cuda')
                  if features.is_cuda
                  else torch.device('cpu'))

        if len(features.shape) < 3:
            raise ValueError('`features` needs to be [bsz, n_views, ...],'
                             'at least 3 dimensions are required')
        if len(features.shape) > 3:
            features = features.view(features.shape[0], features.shape[1], -1)

        batch_size = features.shape[0]
        if labels is not None and mask is not None:
            raise ValueError('Cannot define both `labels` and `mask`')
        elif labels is None and mask is None:
            mask = torch.eye(batch_size, dtype=torch.float32).to(device)
        elif labels is not None:
            labels = labels.contiguous().view(-1, 1)
            if labels.shape[0] != batch_size:
                raise ValueError('Num of labels does not match num of features')
            mask = torch.eq(labels, labels.T).float().to(device)
        else:
            mask = mask.float().to(device)

        contrast_count = features.shape[1]
        contrast_feature = torch.cat(torch.unbind(features, dim=1), dim=0)
        if self.contrast_mode == 'one':
            anchor_feature = features[:, 0]
            anchor_count = 1
        elif self.contrast_mode == 'all':
            anchor_feature = contrast_feature
            anchor_count = contrast_count
        else:
            raise ValueError('Unknown mode: {}'.format(self.contrast_mode))

        # compute logits
        anchor_dot_contrast = torch.div(
            torch.matmul(anchor_feature, contrast_feature.T),
            self.temperature)
        # for numerical stability
        logits_max, _ = torch.max(anchor_dot_contrast, dim=1, keepdim=True)
        logits = anchor_dot_contrast - logits_max.detach()

        # tile mask
        mask = mask.repeat(anchor_count, contrast_count)
        # mask-out self-contrast cases
        logits_mask = torch.scatter(
            torch.ones_like(mask),
            1,
            torch.arange(batch_size * anchor_count).view(-1, 1).to(device),
            0
        )
        mask = mask * logits_mask

        # compute log_prob
        exp_logits = torch.exp(logits) * logits_mask
        log_prob = logits - torch.log(exp_logits.sum(1, keepdim=True))

        # compute mean of log-likelihood over positive
        # mean_log_prob_pos = (mask * log_prob).sum(1) / mask.sum(1)  # ğŸªµ ê¸°ì¡´: ì–‘ì„± 0ì¼ ë•Œ NaN ë°œìƒ
        
        # ğŸŒ½ ì–‘ì„± 0 ì•µì»¤ ì•ˆì „ ì²˜ë¦¬
        pos_per_row = mask.sum(1)  # ğŸŒ½ [B_total]
        valid = pos_per_row > 0    # ğŸŒ½ ì–‘ì„± ìˆëŠ” ì•µì»¤ë§Œ
        num = (mask * log_prob).sum(1)  # ğŸŒ½
        
        mean_log_prob_pos = torch.zeros_like(pos_per_row)  # ğŸŒ½
        mean_log_prob_pos[valid] = num[valid] / pos_per_row[valid].clamp_min(1)  # ğŸŒ½

        # loss
        # loss = - (self.temperature / self.base_temperature) * mean_log_prob_pos  # ğŸªµ
        # loss = loss.view(anchor_count, batch_size).mean()  # ğŸªµ reshape ì œê±° (valid ì¸ë±ì‹± ë•Œë¬¸)
        
        # ğŸŒ½ valid ì•µì»¤ë§Œ ì†ì‹¤ ê³„ì‚°
        loss = -(self.temperature / self.base_temperature) * mean_log_prob_pos[valid]  # ğŸŒ½
        loss = loss.mean() if valid.any() else torch.zeros([], device=device)  # ğŸŒ½

        return loss


# ğŸ’ ProxyContrastLoss ì¶”ê°€ - í”„ë¡œí† íƒ€ì… ê¸°ë°˜ ëŒ€ì¡° í•™ìŠµ
class ProxyContrastLoss(nn.Module):
    """
    ğŸ’ Prototype-based Contrastive Loss for Continual Learning
    
    í”„ë¡œí† íƒ€ì…(í´ë˜ìŠ¤ ëŒ€í‘œì )ê³¼ì˜ ëŒ€ì¡° í•™ìŠµìœ¼ë¡œ catastrophic forgetting ë°©ì§€
    - ê³ ì •ëœ ì•µì»¤ ì—­í• ë¡œ ë“œë¦¬í”„íŠ¸ ê°ì†Œ
    - Top-K ì„ íƒìœ¼ë¡œ ê³„ì‚° íš¨ìœ¨ì„± í™•ë³´
    - ì •ë‹µ í´ë˜ìŠ¤ ê°•ì œ í¬í•¨ìœ¼ë¡œ ì•ˆì •ì„± ë³´ì¥
    
    Args:
        temperature: ì†Œí”„íŠ¸ë§¥ìŠ¤ ì˜¨ë„ (ë†’ì„ìˆ˜ë¡ ë¶€ë“œëŸ¬ìš´ ë¶„í¬)
        lambda_proxy: ì†ì‹¤ ê°€ì¤‘ì¹˜
        topk: ë¹„êµí•  ìµœëŒ€ í´ë˜ìŠ¤ ìˆ˜
        full_until: ì´ ìˆ˜ ì´í•˜ë©´ ëª¨ë“  í´ë˜ìŠ¤ ì‚¬ìš©
    """
    def __init__(self, temperature=0.15, lambda_proxy=0.3, topk=30, full_until=100):
        super().__init__()
        self.temperature = temperature  # ğŸ’ SupConë³´ë‹¤ ë†’ì€ ì˜¨ë„ (0.15 vs 0.07)
        self.lambda_proxy = lambda_proxy
        self.topk = topk
        self.full_until = full_until
    
    def forward(self, z, y, proto_cache_P, proto_cache_ids):
        """
        ğŸ’ Forward pass
        
        Args:
            z: [B, D] ë°°ì¹˜ ì„ë² ë”© (L2 ì •ê·œí™”ë¨)
            y: [B] ë¼ë²¨
            proto_cache_P: [C, D] í”„ë¡œí† íƒ€ì… í…ì„œ (L2 ì •ê·œí™”ë¨)
            proto_cache_ids: ì •ë ¬ëœ í´ë˜ìŠ¤ ID ë¦¬ìŠ¤íŠ¸
            
        Returns:
            ìŠ¤ì¹¼ë¼ ì†ì‹¤ê°’
        """
        # ğŸ’ í”„ë¡œí† íƒ€ì…ì´ ì—†ìœ¼ë©´ 0 ë°˜í™˜ (ì´ˆê¸° ìƒíƒœ)
        if proto_cache_P is None or len(proto_cache_ids) == 0:
            return torch.tensor(0.0, device=z.device)
        
        B = z.size(0)  # ë°°ì¹˜ í¬ê¸°
        C = proto_cache_P.size(0)  # í´ë˜ìŠ¤ ìˆ˜
        
        # ğŸ’ í´ë˜ìŠ¤ ID â†’ ì¸ë±ìŠ¤ ë§¤í•‘
        id2idx = {cid: i for i, cid in enumerate(proto_cache_ids)}
        
        # ğŸ’ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚° (ì´ë¯¸ ì •ê·œí™”ëœ ë²¡í„°ë“¤)
        # AMP ëŒ€ì‘
        if z.dtype == torch.float16:
            sim = (z.float() @ proto_cache_P.float().t()) / self.temperature
        else:
            sim = (z @ proto_cache_P.t()) / self.temperature  # [B, C]
        
        # ğŸ’ Top-K ì„ íƒ ë˜ëŠ” ì „ì²´ ì‚¬ìš©
        if C <= self.full_until or self.topk >= C:
            # í´ë˜ìŠ¤ê°€ ì ìœ¼ë©´ ì „ì²´ ì‚¬ìš©
            sel_idx = torch.arange(C, device=z.device).unsqueeze(0).expand(B, -1)  # [B, C]
            sel_sim = sim
        else:
            # ğŸ’ Top-K í´ë˜ìŠ¤ë§Œ ì„ íƒ (ê³„ì‚° íš¨ìœ¨ì„±)
            sel_sim, sel_idx = sim.topk(self.topk, dim=1)  # [B, K]
            
            # ğŸ’ ì •ë‹µ í´ë˜ìŠ¤ ê°•ì œ í¬í•¨ (ì¤‘ìš”!)
            for i in range(B):
                true_label = y[i].item()
                if true_label in id2idx:
                    true_idx = id2idx[true_label]
                    
                    # ğŸ’ í…ì„œ in ì—°ì‚° ë²„ê·¸ ë°©ì§€ - .any() ì‚¬ìš©
                    exists = (sel_idx[i] == true_idx).any()
                    if not exists:
                        # Top-Kì— ì—†ìœ¼ë©´ ë§ˆì§€ë§‰ ìë¦¬ë¥¼ ì •ë‹µìœ¼ë¡œ êµì²´
                        sel_sim[i, -1] = sim[i, true_idx]
                        sel_idx[i, -1] = true_idx
        
        # ğŸ’ Cross Entropy ì†ì‹¤ ê³„ì‚°
        losses = []
        for i in range(B):
            true_label = y[i].item()
            
            # í”„ë¡œí† íƒ€ì…ì´ ì—†ëŠ” ìƒˆ í´ë˜ìŠ¤ëŠ” ìŠ¤í‚µ
            if true_label not in id2idx:
                continue
            
            true_idx = id2idx[true_label]
            
            # ğŸ’ ì„ íƒëœ í´ë˜ìŠ¤ ì¤‘ ì •ë‹µ ìœ„ì¹˜ ì°¾ê¸°
            pos_mask = (sel_idx[i] == true_idx)
            if not pos_mask.any():
                continue  # ì •ë‹µì´ ì—†ìœ¼ë©´ ìŠ¤í‚µ (ë°©ì–´ ì½”ë“œ)
            
            pos_in_topk = pos_mask.nonzero(as_tuple=True)[0][0]
            
            # ğŸ’ Softmax cross entropy
            log_probs = sel_sim[i].log_softmax(dim=0)  # [K or C]
            losses.append(-log_probs[pos_in_topk])
        
        # ğŸ’ í‰ê·  ì†ì‹¤ ë°˜í™˜
        if losses:
            return self.lambda_proxy * torch.stack(losses).mean()
        else:
            return torch.tensor(0.0, device=z.device)
    
    def update_lambda(self, new_lambda):
        """ğŸ’ Lambda ê°’ ë™ì  ì—…ë°ì´íŠ¸ (ìŠ¤ì¼€ì¤„ë§ìš©)"""
        self.lambda_proxy = new_lambda